{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4bac50",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21981826",
   "metadata": {},
   "source": [
    "## Load Data <br>\n",
    "\n",
    "REF: Document Loader[PDF, WebPage, CSV like]: https://python.langchain.com/docs/how_to/#document-loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688dc32",
   "metadata": {},
   "source": [
    "### Web Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73e0cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c638be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=('post-title', 'post-header', 'post-content'))\n",
    "loader = WebBaseLoader(web_path=url, bs_kwargs={'parse_only': bs4_strainer})\n",
    "\n",
    "web_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3b4fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\n\\nComparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\n\\nComparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\n\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\n\\nIllustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\n\\nPseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b442e978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Overview of a LLM-powered autonomous agent system.\n",
      "\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\n",
      "The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)\n",
      "\n",
      "\n",
      "Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\n",
      "\n",
      "In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\n",
      "Reflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\n",
      "\n",
      "\n",
      "Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "\n",
      "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\n",
      "To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\n",
      "The training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\n",
      "\n",
      "\n",
      "After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
      "\n",
      "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n",
      "\n",
      "\n",
      "Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\n",
      "\n",
      "The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\n",
      "In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\n",
      "In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\n",
      "\n",
      "\n",
      "Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\n",
      "\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\n",
      "\n",
      "\n",
      "Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n",
      "\n",
      "\n",
      "Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Categorization of human memory.\n",
      "\n",
      "We can roughly consider the following mappings:\n",
      "\n",
      "Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\n",
      "Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\n",
      "Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
      "\n",
      "Maximum Inner Product Search (MIPS)#\n",
      "The external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)​ algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\n",
      "A couple common choices of ANN algorithms for fast MIPS:\n",
      "\n",
      "LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\n",
      "ANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\n",
      "HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\n",
      "FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\n",
      "ScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\tilde{x}_i$ such that the inner product $\\langle q, x_i \\rangle$ is as similar to the original distance of $\\angle q, \\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\n",
      "\n",
      "\n",
      "\n",
      "Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\n",
      "\n",
      "Check more MIPS algorithms and performance comparison in ann-benchmarks.com.\n",
      "Component Three: Tool Use#\n",
      "Tool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\n",
      "\n",
      "\n",
      "A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\n",
      "\n",
      "MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\n",
      "They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\n",
      "Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\n",
      "ChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\n",
      "HuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\n",
      "\n",
      "\n",
      "Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
      "\n",
      "The system comprises of 4 stages:\n",
      "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
      "Instruction:\n",
      "\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "\n",
      "(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\n",
      "Instruction:\n",
      "\n",
      "Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\n",
      "\n",
      "(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "\n",
      "With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\n",
      "\n",
      "(4) Response generation: LLM receives the execution results and provides summarized results to users.\n",
      "To put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\n",
      "API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\n",
      "\n",
      "\n",
      "Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\n",
      "\n",
      "In the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\n",
      "\n",
      "Whether an API call is needed.\n",
      "Identify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\n",
      "Response based on the API results: the model can choose to refine and call again if results are not satisfied.\n",
      "\n",
      "This benchmark evaluates the agent’s tool use capabilities at three levels:\n",
      "\n",
      "Level-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\n",
      "Level-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\n",
      "Level-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\n",
      "\n",
      "Case Studies#\n",
      "Scientific Discovery Agent#\n",
      "ChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\n",
      "\n",
      "The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\n",
      "It is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\n",
      "\n",
      "One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\n",
      "Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\n",
      "For example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\n",
      "\n",
      "inquired about current trends in anticancer drug discovery;\n",
      "selected a target;\n",
      "requested a scaffold targeting these compounds;\n",
      "Once the compound was identified, the model attempted its synthesis.\n",
      "\n",
      "They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\n",
      "Generative Agents Simulation#\n",
      "Generative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\n",
      "The design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\n",
      "\n",
      "Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
      "\n",
      "Each element is an observation, an event directly provided by the agent.\n",
      "- Inter-agent communication can trigger new natural language statements.\n",
      "\n",
      "\n",
      "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
      "\n",
      "Recency: recent events have higher scores\n",
      "Importance: distinguish mundane from core memories. Ask LM directly.\n",
      "Relevance: based on how related it is to the current situation / query.\n",
      "\n",
      "\n",
      "Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n",
      "\n",
      "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "\n",
      "This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n",
      "Proof-of-Concept Examples#\n",
      "AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\n",
      "Here is the system message used by AutoGPT, where {{...}} are user inputs:\n",
      "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
      "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "\n",
      "GOALS:\n",
      "\n",
      "1. {{user-provided goal 1}}\n",
      "2. {{user-provided goal 2}}\n",
      "3. ...\n",
      "4. ...\n",
      "5. ...\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "5. Use subprocesses for commands that will not terminate within a few minutes\n",
      "\n",
      "Commands:\n",
      "1. Google Search: \"google\", args: \"input\": \"<search>\"\n",
      "2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\n",
      "3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\n",
      "4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\n",
      "5. List GPT Agents: \"list_agents\", args:\n",
      "6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\n",
      "7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\n",
      "8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "9. Read file: \"read_file\", args: \"file\": \"<file>\"\n",
      "10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\n",
      "11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\n",
      "12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\n",
      "13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\n",
      "14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\n",
      "15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\n",
      "16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\n",
      "17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\n",
      "18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\n",
      "19. Do Nothing: \"do_nothing\", args:\n",
      "20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\n",
      "\n",
      "Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
      "\n",
      "You should only respond in JSON format as described below\n",
      "Response Format:\n",
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"thought\",\n",
      "        \"reasoning\": \"reasoning\",\n",
      "        \"plan\": \"- short bulleted\\n- list that conveys\\n- long-term plan\",\n",
      "        \"criticism\": \"constructive self-criticism\",\n",
      "        \"speak\": \"thoughts summary to say to user\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"command name\",\n",
      "        \"args\": {\n",
      "            \"arg name\": \"value\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Ensure the response can be parsed by Python json.loads\n",
      "GPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\n",
      "Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Summary of areas that need clarification:\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\n2. Details about the MVC components (e.g. which components are in each file)\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\n\\nClarifying question:\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\n\\nIs anything else unclear? If yes, only answer in the form:\\n{remaining unclear areas} remaining questions.\\n{Next question}\\nIf everything is sufficiently clear, only answer \\\"Nothing more to clarify.\\\".\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Remaining unclear areas: 2 remaining questions.\\nCan you provide more information about how the MVC components are split into separate files?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\n",
      "  }\n",
      "]\n",
      "Then after these clarification, the agent moved into the code writing mode with a different system message.\n",
      "System message:\n",
      "\n",
      "You will get instructions for code to write.\n",
      "You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Make sure that every detail of the architecture is, in the end, implemented as code.\n",
      "Think step by step and reason yourself to the right decisions to make sure we get it right.\n",
      "You will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\n",
      "Then you will output the content of each file including ALL code.\n",
      "Each file must strictly follow a markdown code block format, where the following tokens must be replaced such that\n",
      "FILENAME is the lowercase file name including the file extension,\n",
      "LANG is the markup code block language for the code’s language, and CODE is the code:\n",
      "FILENAME\n",
      "CODE\n",
      "You will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\n",
      "Please note that the code should be fully functional. No placeholders.\n",
      "Follow a language and framework appropriate best practice file naming convention.\n",
      "Make sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\n",
      "Ensure to implement all code, if you are unsure, write a plausible implementation.\n",
      "Include module dependency or package manager dependency definition file.\n",
      "Before you finish, double check that all parts of the architecture is present in the files.\n",
      "Useful to know:\n",
      "You almost always put different classes in different files.\n",
      "For Python, you always create an appropriate requirements.txt file.\n",
      "For NodeJS, you always create an appropriate package.json file.\n",
      "You always add a comment briefly describing the purpose of the function definition.\n",
      "You try to add comments explaining very complex bits of logic.\n",
      "You always follow the best practices for the requested languages in terms of describing the code written as a defined\n",
      "package/project.\n",
      "Python toolbelt preferences:\n",
      "\n",
      "pytest\n",
      "dataclasses\n",
      "\n",
      "\n",
      "Conversatin samples:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\n\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\n\\n\\nPython toolbelt preferences:\\n- pytest\\n- dataclasses\\n\"\n",
      "  },\n",
      " #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"Assumptions:\\n1. Model: The model will contain the game's data, such as level information, character states, and enemy positions.\\n2. View: The view will handle the game's visuals, including rendering the game objects, backgrounds, and updating the display.\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\n\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Please now remember the steps:\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code's language, and CODE is the code:\\n\\nFILENAME\\n```LANG\\nCODE\\n```\\n\\nPlease note that the code should be fully functional. No placeholders.\\n\\nYou will start with the \\\"entrypoint\\\" file, then go to the ones that are imported by that file, and so on.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\n\"\n",
      "  }\n",
      "]\n",
      "Challenges#\n",
      "After going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\n",
      "\n",
      "\n",
      "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
      "\n",
      "\n",
      "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
      "\n",
      "\n",
      "Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\n",
      "\n",
      "\n",
      "Citation#\n",
      "Cited as:\n",
      "\n",
      "Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\n",
      "\n",
      "Or\n",
      "@article{weng2023agent,\n",
      "  title   = \"LLM-powered Autonomous Agents\",\n",
      "  author  = \"Weng, Lilian\",\n",
      "  journal = \"lilianweng.github.io\",\n",
      "  year    = \"2023\",\n",
      "  month   = \"Jun\",\n",
      "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
      "}\n",
      "References#\n",
      "[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n",
      "[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\n",
      "[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
      "“ arXiv preprint arXiv:2302.02676 (2023).\n",
      "[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\n",
      "[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\n",
      "[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\n",
      "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
      "[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\n",
      "[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\n",
      "[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\n",
      "[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\n",
      "[12] Parisi et al. “TALM: Tool Augmented Language Models”\n",
      "[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\n",
      "[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\n",
      "[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\n",
      "[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\n",
      "[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\n",
      "[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\n",
      "[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\n",
      "[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\n",
      "[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\n",
      "' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
     ]
    }
   ],
   "source": [
    "print(web_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91dbb0e",
   "metadata": {},
   "source": [
    "### PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b33254fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c33b2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = r\"A:\\Indian_AI_Production_5\\agentic_ai\\agentic_ai_projects\\data\\pdfs\\2509.03680v1.pdf\"\n",
    "\n",
    "pdf_loader = PyPDFLoader(file_path)\n",
    "\n",
    "pdf_data = pdf_loader.load()\n",
    "len(pdf_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067dec77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1'}, page_content='LuxDiT: Lighting Estimation with Video Diffusion\\nTransformer\\nRuofan Liang1,2,3 Kai He1,2,3 Zan Gojcic1 Igor Gilitschenski2,3\\nSanja Fidler1,2,3 Nandita Vijaykumar2,3† Zian Wang1,2,3†\\n1NVIDIA 2University of Toronto 3Vector Institute\\nAbstract\\nEstimating scene lighting from a single image or video remains a longstand-\\ning challenge in computer vision and graphics. Learning-based approaches are\\nconstrained by the scarcity of ground-truth HDR environment maps, which are\\nexpensive to capture and limited in diversity. While recent generative models\\noffer strong priors for image synthesis, lighting estimation remains difficult due\\nto its reliance on indirect visual cues, the need to infer global (non-local) con-\\ntext, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a\\nnovel data-driven approach that fine-tunes a video diffusion transformer to gen-\\nerate HDR environment maps conditioned on visual input. Trained on a large\\nsynthetic dataset with diverse lighting conditions, our model learns to infer il-\\nlumination from indirect visual cues and generalizes effectively to real-world\\nscenes. To improve semantic alignment between the input and the predicted en-\\nvironment map, we introduce a low-rank adaptation finetuning strategy using a\\ncollected dataset of HDR panoramas. Our method produces accurate lighting\\npredictions with realistic angular high-frequency details, outperforming existing\\nstate-of-the-art techniques in both quantitative and qualitative evaluations. Project\\npage: https://research.nvidia.com/labs/toronto-ai/LuxDiT/\\n1 Introduction\\nIn physically-based rendering, lighting plays a central role in shaping the appearance—how objects\\ncast shadows, reflect, and appear integrated within a scene. From virtual object insertion and\\naugmented reality to synthetic data generation, many downstream tasks rely on estimating scene\\nillumination. Yet inferring lighting from casually captured images or video remains an open challenge.\\nA common representation of the scene illumination is the high-dynamic-range (HDR) environment\\nmap, which describes incoming light intensity from all directions. HDR maps can be acquired by\\nusing light probes or multi-exposure panoramas, requiring specialized setups that are impractical for\\neveryday use [9]. To overcome this, several learning-based methods that estimate environment maps\\ndirectly from casually captured LDR images or videos have been proposed [15, 16, 32, 73]. However,\\nthese methods typically depend on paired datasets of input images or videos and HDR environment\\nmaps, leading to a chicken-and-egg problem: a large collection of HDR environment maps is needed\\nto train a model that aims to alleviate the need for acquiring such expensive data in the first place.\\nRecently, generative diffusion models have demonstrated strong capabilities in modeling complex\\nimage distributions. DiffusionLight [44] demonstrated that pretrained text-to-image models encode\\nimplicit knowledge of illumination, which can be cleverly extracted by inpainting a virtual chrome\\nball into an image, generating plausible appearances under varying exposure settings. However,\\nwithout task-specific fine-tuning, the inpainting priors of pre-trained diffusion models are insufficient\\n† Joint Advising\\nPreprint.\\narXiv:2509.03680v1  [cs.GR]  3 Sep 2025')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef6fb942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\n\\nComparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\n\\nComparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\n\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\n\\nIllustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\n\\nPseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1'}, page_content='LuxDiT: Lighting Estimation with Video Diffusion\\nTransformer\\nRuofan Liang1,2,3 Kai He1,2,3 Zan Gojcic1 Igor Gilitschenski2,3\\nSanja Fidler1,2,3 Nandita Vijaykumar2,3† Zian Wang1,2,3†\\n1NVIDIA 2University of Toronto 3Vector Institute\\nAbstract\\nEstimating scene lighting from a single image or video remains a longstand-\\ning challenge in computer vision and graphics. Learning-based approaches are\\nconstrained by the scarcity of ground-truth HDR environment maps, which are\\nexpensive to capture and limited in diversity. While recent generative models\\noffer strong priors for image synthesis, lighting estimation remains difficult due\\nto its reliance on indirect visual cues, the need to infer global (non-local) con-\\ntext, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a\\nnovel data-driven approach that fine-tunes a video diffusion transformer to gen-\\nerate HDR environment maps conditioned on visual input. Trained on a large\\nsynthetic dataset with diverse lighting conditions, our model learns to infer il-\\nlumination from indirect visual cues and generalizes effectively to real-world\\nscenes. To improve semantic alignment between the input and the predicted en-\\nvironment map, we introduce a low-rank adaptation finetuning strategy using a\\ncollected dataset of HDR panoramas. Our method produces accurate lighting\\npredictions with realistic angular high-frequency details, outperforming existing\\nstate-of-the-art techniques in both quantitative and qualitative evaluations. Project\\npage: https://research.nvidia.com/labs/toronto-ai/LuxDiT/\\n1 Introduction\\nIn physically-based rendering, lighting plays a central role in shaping the appearance—how objects\\ncast shadows, reflect, and appear integrated within a scene. From virtual object insertion and\\naugmented reality to synthetic data generation, many downstream tasks rely on estimating scene\\nillumination. Yet inferring lighting from casually captured images or video remains an open challenge.\\nA common representation of the scene illumination is the high-dynamic-range (HDR) environment\\nmap, which describes incoming light intensity from all directions. HDR maps can be acquired by\\nusing light probes or multi-exposure panoramas, requiring specialized setups that are impractical for\\neveryday use [9]. To overcome this, several learning-based methods that estimate environment maps\\ndirectly from casually captured LDR images or videos have been proposed [15, 16, 32, 73]. However,\\nthese methods typically depend on paired datasets of input images or videos and HDR environment\\nmaps, leading to a chicken-and-egg problem: a large collection of HDR environment maps is needed\\nto train a model that aims to alleviate the need for acquiring such expensive data in the first place.\\nRecently, generative diffusion models have demonstrated strong capabilities in modeling complex\\nimage distributions. DiffusionLight [44] demonstrated that pretrained text-to-image models encode\\nimplicit knowledge of illumination, which can be cleverly extracted by inpainting a virtual chrome\\nball into an image, generating plausible appearances under varying exposure settings. However,\\nwithout task-specific fine-tuning, the inpainting priors of pre-trained diffusion models are insufficient\\n† Joint Advising\\nPreprint.\\narXiv:2509.03680v1  [cs.GR]  3 Sep 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2'}, page_content='Input Image Lighting Estimation Virtual Object Insertion\\nFigure 1: LuxDiT is a generative lighting estimation model that predicts high-quality HDR envi-\\nronment maps from visual input. It produces accurate lighting while preserving scene semantics,\\nenabling realistic virtual object insertion under diverse conditions.\\nfor producing reliable lighting estimates in a single inference and cannot directly generate HDR\\noutputs. As a result, DiffusionLight relies on an expensive test-time ensemble strategy to improve\\nrobustness. Moreover, sampling multiple exposures through separate inference passes introduces\\ninconsistencies and limits the dynamic range of the reconstructed illumination.\\nIn this work, we formulate lighting estimation as a conditional generative task and propose LuxDiT,\\na neural lighting predictor trained on synthetic data and adapted to real-world scenes. Conditioned on\\nvisual input, our approach fine-tunes a diffusion transformer (DiT) to synthesize HDR panoramas\\nfrom noise. Unlike pixel-aligned tasks, lighting estimation requires global reasoning over scene\\ncontext. DiTs are particularly suited to this task: their attention-based architecture supports global\\ncontext aggregation, and their generative priors facilitate reasoning from indirect cues such as shading\\nand reflections.\\nTraining such a model requires diverse lighting data. To overcome the lack of real-world HDR lighting\\nsupervision, we construct a large-scale synthetic dataset with randomized geometry, materials, and\\nlighting conditions. Training on this dataset allows the model to learn physically grounded cues for\\nlight direction and intensity. While this imparts general lighting priors, models trained purely on\\nsynthetic data often hallucinate lighting based on dataset priors, producing environment maps that\\nare plausible but semantically mismatched with the input scene. For example, an image of an urban\\nstreet may yield an environment map depicting a rural landscape. To address this, we further apply\\nlow-rank adaptation (LoRA) [23] on a curated set of real HDR panoramas, improving alignment\\nbetween predicted lighting and scene semantics.\\nGiven a single image or video, LuxDiT produces HDR environment maps with accurate direction,\\nintensity, and scene-consistent content. It reduces lighting estimation error by 45% on Laval Outdoor\\nsunlight direction and improves temporal consistency for video input, enabling reliable use in\\ndownstream applications such as virtual object insertion. Our main contributions are:\\n• A DiT-based generative architecture that synthesizes HDR environment maps from visual input.\\n• A LoRA-based fine-tuning strategy using curated HDR panoramas to improve semantic alignment\\nbetween the input scene and predicted illumination.\\n• A large-scale synthetic dataset with randomized geometry, materials, and lighting.\\n2 Related Work\\nLighting estimation aims to infer environment illumination from input imagery, and is critical for\\nphotorealistic rendering and virtual object insertion. Early learning-based methods treat lighting\\nestimation as a supervised regression problem, predicting spherical lobes [16, 32, 71, 68], parametric\\nsources [65, 14], or low-resolution environment maps [15, 49, 73, 51] directly from a single image.\\nThese models are trained on paired data obtained from real-world captures [15, 49, 57] or synthetic\\nrendering [32, 73, 51]. However, their performance often degrades in complex, in-the-wild scenes\\ndue to limited diversity in the training data.\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3'}, page_content='Recent methods incorporate generative priors to address the ambiguity of scene illumination. Style-\\nLight [54] fine-tunes a StyleGAN to generate LDR and HDR panoramas from latent codes, using\\nGAN inversion at test time. However, its performance hinges on inversion quality and often breaks\\nsemantic alignment on out-of-domain inputs. EverLight [8] regresses a parametric lighting estimate\\nand refines it with a GAN to add high-frequency detail, but relies on pseudo-labeled HDR data and\\nstruggles with complex or bright lighting. DiffusionLight [ 44] uses a diffusion model to inpaint\\na virtual chrome ball under multiple exposures, merging them into an HDR map. While visually\\nplausible, this multi-stage process yields distorted panoramas and limited dynamic range.\\nInverse renderingrecovers scene properties such as geometry, material reflectance, and illumination\\nfrom image observations. Lighting estimation is often treated as a subcomponent of this broader task,\\nwith prior work jointly estimating lighting alongside depth, normals, and albedo. Learning-based\\napproaches [48, 32, 58] typically leverage physics-based constraints and use re-rendering losses to\\nsupervise predictions. However, these methods often assume simplified reflectance models such as\\nLambertian shading, which limits their ability to handle complex lighting effects.\\nOptimization-based methods leverage differentiable rendering [ 4, 70, 69, 6, 59, 41, 18, 33] to\\njointly optimize lighting parameters and other scene attributes through photometric losses and\\nregularization terms. Some approaches [ 30] follow a decomposition-then-optimization strategy:\\nestimating geometry and albedo first, then solving for lighting via optimization. Other works\\nalso explore priors from proxy geometry [ 64] or pretrained general models [ 37, 35, 42]. The\\noptimization-based pipelines often require dense multi-view captures or known proxy geometry, and\\ninvolve expensive test-time optimization procedures. In contrast, our method directly predicts HDR\\nillumination in a feed-forward manner without requiring scene geometry or iterative inference.\\nDiffusion model priors.Diffusion models (DMs) have emerged as a powerful class of generative\\nmodels in high-fidelity image [45, 2, 46, 7] and video synthesis [21, 72, 3, 62, 1]. Beyond generation,\\npretrained DMs have been adapted to perception tasks through task-specific finetuning on carefully\\ncurated datasets [61, 38, 19], showing strong results on spatially aligned predictions such as depth [29,\\n24, 28], surface normals [13, 63, 34], albedo [11, 30, 67, 34], and material properties [30, 67, 34, 42].\\nAdapting DMs to non-local tasks like lighting introduces new modeling challenges, as outputs such\\nas HDR panoramas are not spatially-aligned with the input.\\n3 Preliminaries: Diffusion Models\\nDiffusion models learn to approximate a data distribution pdata(x) through iterative denoising. Fol-\\nlowing DDPM [20], a forward process progressively adds Gaussian noise to a data samplex0 ∼ pdata,\\nproducing a noisy version at timestep t ∈ [1, T] as: xt = √¯αtx0 + √1 − ¯αtϵ, where ϵ ∼ N(0, I)\\nand ¯αt defines the noise schedule. During training, a neural network µθ learns to reverse this process\\nby minimizing:\\nEx0∼pdata(x),t∼pt,ϵ∼N(0,I)\\n\\x02\\n∥µθ(xt; c, t) − y∥2\\n2\\n\\x03\\n, (1)\\nwhere c represents optional conditioning inputs. The denoising target y varies by formulation, and\\ncan be the noise ϵ [20], the v-prediction √¯αtϵ − √1 − ¯αtx0 [47], or the clean signal x0 itself [27].\\nAt inference time, samples are generated by denoising an initial Gaussian sample through a fixed\\nnumber of reverse steps. In this paper, we build on CogVideoX [62], a latent video diffusion model\\ntrained on compressed video representations. A pretrained auto-encoder pair {E, D} maps RGB\\nvideos to and from a latent space, such that E(x) = z and D(z) ≈ x. All diffusion training and\\ngeneration is performed in this lower-dimensional latent space to reduce memory and computational.\\n4 Method\\nWe propose LuxDiT, a diffusion-based generative framework for estimating high-dynamic-range\\n(HDR) environment maps from a single image or video. We tailor a recent video diffusion transformer\\narchitecture [62] for lighting estimation, by jointly processing denoising targets (environment lighting)\\nand condition tokens (LDR input images) through self-attention layers. Since a single image can be\\ntreated as a one-frame video, we refer to both inputs uniformly as input video in the remainder of this\\nsection. An overview of the architecture is shown in Figure 2. In the following sections, we describe\\nthe model design, data sources, and training procedure.\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4'}, page_content='Diffusion \\nTransformer\\nLoRA\\nNoneStage 1\\nStage 2\\nChannel \\nconcat.\\nInput Image/Video !\\nFrozen\\n\"!\"#\\n\"$%&\\n\"$!#\\nInput Env \\nPred. Env\\n#\"$!#\\n#\"$%&\\nHDR Env #\"\\nHDR \\nFusion \\nMLP\\nToken concat.\\nFigure 2: Method Overview.Given an input image or video I, LuxDiT predicts an environment map E as two\\ntone-mapped representations, Eldr and Elog, guided by a directional map Edir. Environment maps are encoded\\nwith a V AE, and the resulting latents are concatenated and jointly processed with visual input by a DiT. The\\noutputs Eldr and Elog are decoded and fused by a lightweight MLP to reconstruct the final HDR panorama.\\n4.1 Model Design\\nWe formulate HDR environment map estimation as a conditional denoising task. Given an input\\nvideo I ∈ RL×H×W×3 with L frames, the model generates a corresponding sequence of 360◦ HDR\\npanoramas E ∈ RL×He×We×3.\\nTwo core challenges arise: (1) standard V AEs used in latent diffusion models are trained on LDR\\nimages and cannot faithfully encode HDR content, and (2) the output panoramas are not spatially\\naligned with the input, requiring flexible conditioning mechanisms. We address these challenges\\nusing a dual-tonemapping HDR representation, token-based conditioning, and a unified transformer\\narchitecture that jointly denoises two latent representations of lighting.\\nHDR lighting representation.Realistic lighting involves high-intensity components such as the sun\\nor artificial sources, with radiance values often exceeding 100 or 1,000. Representing this range in\\nlatent space is non-trivial: standard V AEs are trained on[0, 1]-normalized LDR images and cannot\\nreconstruct such dynamic content, and retraining on HDR data is impractical due to data scarcity\\nInspired by prior works [26, 34], we represent each HDR panorama E using two complementary\\ntonemapped representations:\\nEldr = E\\n1 + E ·\\n\\x12\\n1 + E\\nM2\\nldr\\n\\x13\\n; Elog = log(1 + E)\\nlog(1 + Mlog) (2)\\nwhere Eldr is a standard Reinhard tonemapping and Elog captures normalized log-intensity. We set\\nMldr = 16 and Mlog = 10,000. Both outputs are clipped to [0, 1] before V AE encoding.\\nAt inference time, the HDR environment map is reconstructed using a lightweight MLP ψ:\\nˆE = ψ (Eldr, Elog) . (3)\\nDiffusion latents. Our model builds on a transformer-based diffusion model µθ, adapted to predict\\nHDR environment maps from visual input. The model operates in latent space and jointly denoises\\ntwo tonemapped representations of the HDR lighting.\\nThe tonemapped inputs Eldr and Elog are encoded by the pretrained V AE into latent tensors[zldr, zlog]\\nwith shape as Rl×he×we×C. These are concatenated along the channel dimension to form the diffusion\\ntarget z = [zldr, zlog] ∈ Rl×he×we×2C. The input and output projection layers of the diffusion network\\nµθ are extended to accommodate the increased channel dimension.\\nConditioning visual input in DiT.Accurate lighting estimation requires the model to extract fine-\\ngrained shading cues from the input image, such as shadow orientation, surface reflections, and\\nspecular highlights. Unlike pixel-aligned image-to-image translation tasks, we empirically observe\\nthat concatenating conditions to the noisy latents leads to poor performance (see Table 7), indicating\\nthe need for a more flexible conditioning mechanism.\\nTo this end, we adopt a fully attention-based architecture for the input video conditions. Specifically,\\nwe encode the input video I ∈ RL×H×W×3 into a latent tensor E(I) ∈ Rl×h×w×C using the pre-\\ntrained V AE encoder, and flatten it into a token sequencec ∈ Rlhw×C. To help the model distinguish\\nbetween condition tokens and denoising targets, we apply separate adaptive layer normalization\\n(AdaLN) modules [43, 62] to each token type at every transformer block.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5'}, page_content='Directional embedding. To improve angular continuity in the predicted panoramas, we inject\\ndirectional information into the model. Specifically, we construct a direction map of unit vectors\\nEdir that encodes per-pixel lighting directions in the camera coordinate system. This map is passed\\nthrough the same V AE encoderE, then projected and fused into the noise tokens using channel-wise\\nconcatenation before the transformer blocks. During training, we apply random horizontal rotations\\nto Edir to encourage rotational equi-variance and robust directional encoding.\\nConditioned denoising process.To put it together, at each denoising timestep t, the model receives\\na noisy latent zt = [zldr\\nt , zlog\\nt ] and predicts the corresponding clean latents conditioned on visual\\ninput as µθ(zt; c, t). This transformer-based design allows the model to propagate indirect lighting\\ncues—such as shadows and reflections—through global self-attention, enabling lighting prediction\\nthat is both scene-consistent and directionally accurate.\\n4.2 Data Strategy\\nSupervised training of our model requires paired data in the form (I, Eldr, Elog), where I is an LDR\\ninput and Eldr, Elog are tonemapped versions of the target HDR environment map. To overcome the\\nscarcity of real-world HDR annotations, we leverage three complementary data sources: synthetic\\nrenderings, HDR panorama images, and LDR panoramic videos.\\nSynthetic rendering data.To supervise lighting prediction using physically accurate visual cues,\\nwe generate synthetic data by rendering randomized 3D scenes lit by HDR environment maps. Each\\nscene consists of (i) a ground plane with randomly assigned PBR materials, (ii) 3D objects sampled\\nfrom Objaverse [10], and (iii) simple geometric primitives such as spheres, cubes, and cylinders\\nwith varied materials. We render multiple frames per scene with randomized camera trajectories and\\nenvironment map rotations. Despite their simplicity, these scenes exhibit diverse lighting effects,\\nincluding cast shadows, specular highlights, and inter-reflections, all paired with ground-truth HDR\\nillumination. Empirically, we find this data is critical for enabling the model to learn accurate shading\\ncues and light-source location (see Table 7).\\nHDR panorama images. We generate training pairs by sampling perspective crops from HDR\\nenvironment maps with data augmentation. Specifically, given a panorama, we randomly sample\\ncamera parameters including azimuth, elevation, field of view, and exposure scale. These parameters\\ndefine a virtual pinhole camera, which we use to project the panorama into an LDR perspective view\\nI. The corresponding HDR environment map serves as the ground truth lighting target E. To support\\ntemporal training, we extend this procedure to generate multi-frame sequences by smoothly varying\\nthe camera pose over time.\\nLDR panorama videos.To enable the generation of dynamic panorama environment maps, we also\\nincorporate training data from LDR panoramic videos. Although ground-truth HDR environment\\nmaps are not available for this source, we use it in the form (I, Eldr, ∅), where Eldr is derived using\\ntonemapping and ∅ indicates the absence of log-space intensity. The panoramic video is projected\\ninto a perspective-view video using randomized camera parameters, following the same procedure as\\nabove. Despite the lack of HDR intensity, this data improves robustness and temporal consistency\\nby exposing the model to natural image statistics, motion patterns, and diverse real-world lighting\\nconditions. We use 2,000 panoramic videos from the WEB360 dataset [56] for training, and hold out\\n114 videos for evaluation.\\n4.3 Training Scheme\\nWe adopt a two-stage training strategy to progressively build the model’s capacity and improve\\ngeneralization. The first stage focuses on learning physically grounded lighting cues from synthetic\\ndata. The second stage adapts the model to real-world distributions through LoRA-based fine-tuning.\\nStage I: Synthetic supervised training.We begin by training the model on the synthetic rendering\\ndataset described in Section 4.2. This stage enables the model to learn the fundamental relationship\\nbetween image-based shading cues and HDR environment lighting.\\nWe follow the standard DDPM training objective [20] adopted by the CogVideoX base model [62]:\\nLI(θ) = Ez0,ϵ∼N(0,I),t∼U(T)\\n\\x02\\n∥ϵ − µθ(zt, c, t)∥2\\n2\\n\\x03\\n, (4)\\nwhere z0 denotes the clean latent pair [zldr, zlog], and c is the conditioning latent from the input video.\\nDuring training, we randomly drop eitherzldr or zlog with probability p = 0.1 to encourage robustness\\nto missing tonemapped representations.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6'}, page_content='Table 1: Comparison of our method with baselines on three benchmark datasets. The results are\\nreported in terms of scale-invariant RMSE, angular error, and normalized RMSE.\\nDataset Method Scale-invariant RMSE↓ Angular Error↓ Normalized RMSE↓\\nDiffuse Matte Mirror Diffuse Matte Mirror Diffuse Matte Mirror\\nLaval Indoor\\nStyleLight 0.135 0.315 0.552 4.238 4.742 6.781 0.234 0.404 0.511\\nDiffusionLight 0.124 0.325 0.597 2.500 3.421 5.936 0.216 0.361 0.431\\nOurs 0.112 0.297 0.586 2.555 3.526 5.641 0.196 0.341 0.457\\nLaval Outdoor\\nH-G et al. [22] 0.300 0.437 0.587 7.851 8.755 26.052 0.551 0.627 0.740\\nNLFE 0.112 0.234 0.431 4.804 5.279 7.278 0.217 0.331 0.496\\nDiffusionLight 0.083 0.224 0.414 1.936 2.955 5.491 0.167 0.330 0.472\\nOurs 0.068 0.190 0.396 2.018 2.939 5.286 0.137 0.271 0.454\\nPoly Haven\\nStyleLight 0.138 0.336 0.620 3.034 4.272 6.602 0.198 0.344 0.474\\nNLFE 0.159 0.326 0.571 3.305 4.240 5.180 0.224 0.365 0.458\\nDiffusionLight 0.113 0.270 0.519 2.199 3.121 4.104 0.191 0.282 0.391\\nOurs 0.077 0.196 0.442 1.235 1.977 2.783 0.111 0.199 0.323\\nTable 2: Angular error on estimated peak\\nluminance light direction on Laval Outdoor\\nsunny scenes.\\nMethod Peak Angular Error↓Mean Median\\nH-G et al. [22] 52.8 47.8NLFE 52.9 43.5DiffusionLight44.4 32.1Ours 23.7 17.5\\nTable 3: Quantitative comparison with video input. Peak an-\\ngular error (PAE) is used to evaluate PolyHaven-Peak videos.\\nAngular error (AE) on is used to evaluate WEB360 LDR videos.\\nMethod PolyHaven-Peak WEB360\\nPAE Mean↓ PAE Std↓ AE↓ AE Std↓\\nDiffusionLight 19.09 10.31 6.504 0.269\\nOurs (image) 5.74 3.68 5.679 0.382\\nOurs (video) 5.21 1.95 5.218 0.072\\nStage II: Semantic adaptation.After base training, we fine-tune the model to improve semantic\\nalignment between the input appearance and the predicted HDR environment map.\\nThis stage uses real-world data sources, including perspective projections from HDR panoramas and\\nLDR panoramic videos. Since HDR ground truth is not available in the latter, we supervise only\\nthe LDR-tonemapped component. To avoid overfitting and preserve the pretrained model capacity,\\nwe apply parameter-efficient LoRA fine-tuning [ 23], optimizing a small set of injected low-rank\\nparameters ∆θ in the transformer layers:\\nLII(∆θ) = Ez0,ϵ∼N(0,I),t∼U(T)\\n\\x02\\n∥ϵ − µθ+∆θ(zt, c, t)∥2\\n2\\n\\x03\\n, (5)\\n5 Experiments\\n5.1 Experiment Settings\\nImplementation details. We use the pre-trained CogVideoX [ 62] model as our backbone. All\\ntraining is conducted on 16 NVIDIA A100 GPUs. Input resolutions are randomly sampled be-\\ntween 512×512 and 480×720, and output environment map resolutions are between 128×256 and\\n256×512. The image-based model is trained with a batch size of 192 for 12,000 iterations. For video\\ntraining, we use the same spatial resolutions and uniformly sample frame lengths from 9, 17, 25. The\\nvideo model is trained with an average batch size of 48 for an additional 12,000 iterations. LoRA\\nmodules are applied to all attention layers with a rank of 64. We fine-tune the LoRA parameters for\\n5,000 iterations during the adaptation stage. Please refer to supplement for implementation details.\\nDatasets. We evaluate our method on the following three benchmark datasets, covering various\\nindoor and outdoor scenes. 1) Laval Indoor [15]: We use the same set of 289 test HDRIs used by prior\\nworks [44, 54]; 2) Laval Outdoor [22]: We evaluate on 116 sunny HDR panoramas with concentrated\\nsunlight selected from the original dataset; 3) Poly Haven [66]: We select 181 Poly Haven HDRIs\\nnot used during model training to evaluate performance across both indoor and outdoor scenes.\\nMetrics. Following prior works [54, 44], we use three standard metrics for evaluating HDR lighting:\\nscale-invariant root mean square error (si-RMSE) [17], angular error in degrees [31], and normalized\\nRMSE (n-RMSE) [44]. For scenes with concentrated sunlight, we additionally report peak angular\\nerror (PAE) [22, 57], which measures the angular deviation of the predicted peak light direction.\\nBaselines. For indoor scenes, we compare against DiffusionLight [44], StyleLight [54], Weber et\\nal. [60], and EMLight [68], using metrics reported by [44] when applicable. For outdoor scenes, we\\ncompare against DiffusionLight [44], Hold-Geoffroy et al. [22], and NLFE [57].\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7'}, page_content='Input Image GT StyleLight DiffusionLight Ours Input Image GT StyleLight DiffusionLight Ours\\nInput Image GT H-G et al. DiffusionLight Ours Input Image GT H-G et al. DiffusionLight Ours\\nInput Image GT NLFE DiffusionLight Ours Input Image GT NLFE DiffusionLight Ours\\nFigure 3: Qualitative comparison with baseline methods on three benchmark datasets.\\nReference StyleLight DiffusionLight DiPIR Ours\\nFigure 4: Qualitative comparison of virtual object insertion.\\nTable 4: Ablation study on impact\\nof LoRA scale at inference time.\\nLoRA Scale Diffuse↓ Matte↓ Mirror↓\\n0.00 2.98 5.02 6.07\\n0.25 2.09 3.69 4.67\\n0.50 1.52 2.66 3.56\\n0.75 1.22 2.05 2.88\\n1.00 1.17 1.92 2.72\\nTable 5: Ablation study on im-\\npact of camera field-of-view.\\nFOV Diffuse↓ Matte↓ Mirror↓\\n45◦ 1.29 2.14 2.95\\n50◦ 1.26 2.06 2.86\\n60◦ 1.17 1.92 2.72\\n70◦ 1.15 1.85 2.63\\n75◦ 1.13 1.80 2.59\\nTable 6: Ablation study on impact\\nof camera elevation.\\nElevation Diffuse↓ Matte↓ Mirror↓\\n−30◦ 1.70 3.04 3.95\\n−15◦ 1.22 2.05 2.87\\n+00◦ 1.17 1.92 2.72\\n+15◦ 1.28 2.09 2.94\\n+30◦ 1.71 2.59 3.51\\nTable 7: Ablation study on model design choices and training data. We report the angular error with three-\\nspheres protocol.\\nSettings Laval Indoor Poly Haven\\nDiffuse↓ Matte↓ Mirror↓ Diffuse↓ Matte↓ Mirror↓\\nOurs (channel concat.) 7.09 10.04 11.07 7.09 10.04 11.07\\nOurs (w/o synthetic data)4.50 5.14 6.96 1.48 2.08 2.86\\nOurs 2.56 3.53 5.64 1.23 1.98 2.78\\n5.2 Evaluation of Image Lighting Estimation\\nWe follow the evaluation protocol from prior work to render spheres with three representative\\nmaterials (gray-diffuse, silver-matte, and mirror), using the estimated HDR environment map from\\nthe LDR input image [15, 54, 44].\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8'}, page_content='Frame 0 Frame 12 Frame 24 Frame 0 Frame 12 Frame 24\\n0 12 24 0 12 24 0 12 24 0 12 24\\nReference DiffusionLight Reference DiffusionLight\\nOurs image Ours video Ours image Ours video\\nFigure 5: Qualitative comparison of video lighting estimation.\\nTable 1 reports quantitative comparisons on three benchmarks spanning both indoor and outdoor\\nscenes. On the Laval Indoor dataset, our method performs comparably or better than DiffusionLight\\nacross most metrics, despite not using Laval Indoor dataset during training. This dataset exhibits\\na noticeable shift in color and intensity distribution compared to our training set, and our strong\\nperformance demonstrates robust generalization.\\nFrom qualitative comparison shown in Figure 3, DiffusionLight can lose angular high-frequency\\ndetails from the input image due to its distorted representation. In contrast, our estimated environment\\nmaps can recover more high-frequency details while preserving accurate lighting.\\nOn the Laval Outdoor and Poly Haven datasets with a broader dynamic range, our method consistently\\noutperforms prior state-of-the-art methods. Hold-Geoffroy et al. [22] can estimate concentrated\\npeak light source such as sunlight; however, its results do not adapt well to the details of the input\\nimage. NLFE [57] can estimate in-context environment maps, but it often fails to estimate accurate\\nhighlights. DiffusionLight performs better than other baselines, but due to its limited dynamic range,\\nit struggles with outdoor high-intensity light sources.\\nTo further assess directional accuracy, we evaluate the angular error of the peak luminance direction\\non a subset of the Laval Outdoor dataset containing direct sunlight. Table 2 reports the mean and\\nmedian peak angular errors. Our method reduces peak angular error by nearly 50% compared to\\nDiffusionLight, confirming its advantage in capturing accurate light direction—a critical factor for\\ncasting realistic shadows in downstream applications such as object insertion.\\n5.3 Evaluation of Video Lighting Estimation\\nTo evaluate lighting estimation accuracy and consistency on video input, we construct two types of\\ntest sequences:\\n• PolyHaven-Peak: We project 12 unseen Poly Haven panoramas (each with direct sunlight) into\\nvideos using a smooth panning camera. This setting is used to evaluate peak angular error.\\n• WEB360: We randomly select 12 LDR panoramic videos featuring dynamic content from\\nWEB360 and render them into perspective views with fixed horizontal camera motion. This\\nsetting evaluates temporal consistency using chromatic angular error on rendered mirror spheres.\\nEach set contains 12 videos at resolution of480×720 and a length of 25 frames. To quantify temporal\\nconsistency, we compute the standard deviation (std) of per-frame error metrics for each video clip,\\nand average the results across the 12-video set.\\nWe compare our video inference to two baselines: our own image-based inference (applied frame-by-\\nframe) and DiffusionLight [44]. Table 3 reports the results. Our method outperforms DiffusionLight.\\nComparing to Ours (image), video inference achieves higher accuracy and significantly lower\\ntemporal variance, indicating more stable predictions across time.\\nFigure 5 shows qualitative examples of video inference. Both DiffusionLight and our image-based\\nvariant exhibit visible temporal flickering. In contrast, our method produces smooth lighting transi-\\ntions, successfully aligning content across frames and preserving consistent lighting behavior over\\ntime.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9'}, page_content='5.4 Evaluation of Virtual Object Insertion\\nTable 8: Quantitative evaluation of virtual\\nobject insertion. We report the percentage of\\nimages where users preferred Ours over base-\\nlines. A preference > 50% indicates Ours\\noutperforming baselines.\\nMethod RMSE ↓SSIM↑Ours Preferred\\nStyleLight 0.056 0.986 60.6%\\nDiffusionLight 0.057 0.987 60.6%\\nDiPIR 0.048 0.989 54.5%\\nOurs 0.047 0.990 /\\nVirtual object insertion is a key downstream applica-\\ntion of lighting estimation. We evaluate our method on\\nthis task using the benchmark from [35], using 11 HDR\\npanoramas from the Poly Haven dataset [66]. For each\\nscene, a virtual object and a known ground plane are\\nmanually placed into the environment. Each test case\\nincludes an LDR background image rendered from the\\nHDR panorama, along with a posed object and ground\\nplane. A pseudo-ground-truth object insertion is gen-\\nerated by rendering the object using the original HDR\\nenvironment map. This allows for controlled comparison\\nagainst renderings produced using predicted lighting.\\nWe report quantitative metrics in Table 8. In addition, we conduct a user study to assess perceptual\\nquality (details provided in the supplement), and report the percentage of samples where users\\npreferred our results over baseline methods.\\nOur method achieves visual quality comparable to DiPIR and significantly outperforms other baselines.\\nNotably, DiPIR is specialized for object insertion and incorporates additional modules for tone\\nmapping and appearance harmonization. In contrast, our model estimates lighting alone, yet still\\nproduces realistic composite renderings. We include qualitative results in Figure 4.\\n5.5 Ablation Study\\nModel Design and Training DataWe evaluate two model variants to ablate the contributions\\nof our architectural and training design: (1) Channel concatenation: This variant fuses input and\\nenvironment map (resized to match input image) latents along the channel dimension [ 26], and\\nno token-wise concatenation is used. Our two-stage training is also applied. (2) Training without\\nsynthetic data: This variant skips Stage I training and uses only panorama crops for fine-tuning.\\nTable 7 reports angular errors on Laval Indoor and Poly Haven. Channel concatenation significantly\\nunderperforms, confirming the importance of token-level conditioning. Without synthetic pretraining,\\nthe model performs well in-domain (Poly Haven) but degrades out-of-domain (Laval Indoor), showing\\nsynthetic data pre-training is crucial for learning generalized lighting priors.\\nLoRA scale. We vary the LoRA interpolation weight from 0.0 to 1.0 to ablate how fine-tuned LoRA\\naffects the predicted lighting content. Table 4 shows that higher LoRA weights yield lower angular\\nerror on Poly Haven, validating the effectiveness of LoRA for improving semantic alignment.\\nCamera sensitivity. We test robustness to camera variation by rendering crops from Poly Haven\\nunder varying field of view (45◦ to 75◦) and camera elevation (−30◦ to 30◦). Results in Tables 5 and\\n6 show that while extreme viewpoints introduce mild error increases, performance remains stable,\\ndemonstrating robustness to moderate viewpoint shifts.\\n6 Discussion\\nWe introduce LuxDiT, a conditional generative model for estimating HDR scene illumination from\\ncasually captured images and videos. Our approach fine-tunes a video diffusion transformer (DiT)\\nto synthesize HDR environment maps, combining large-scale synthetic data for learning physically\\ngrounded priors with LoRA-based adaptation on real HDR panoramas to improve semantic alignment.\\nExtensive experiments demonstrate that LuxDiT produces accurate, high-frequency, and scene-\\nconsistent lighting predictions from limited visual input.\\nLimitations and future work.While LuxDiT produces high-quality lighting predictions, inference\\nremains computationally intensive due to the iterative nature of diffusion models, limiting its use in\\nreal-time applications. Future work could explore model distillation or more efficient architectures\\nto accelerate inference. Additionally, the resolution of predicted panoramas is limited by data and\\ntraining scale; generating high-resolution outputs for immersive applications will require richer, more\\ndiverse HDR supervision. Looking ahead, with recent progress in joint generative modeling [5, 36],\\nwe see LuxDiT as a step toward unified inverse and forward rendering frameworks, complementing\\nrecent progress in neural forward rendering and G-buffer estimation [ 67, 34]. Future directions\\ninclude joint modeling or co-training of lighting, geometry, and material for general-purpose scene\\nreconstruction and appearance synthesis.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10'}, page_content='References\\n[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay,\\nYongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv\\npreprint arXiv:2501.03575, 2025.\\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis,\\nMiika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. eDiff-I:\\ntext-to-image diffusion models with ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\\n[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and\\nKarsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n[4] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik P.A. Lensch. NeRD:\\nneural reflectance decomposition from image collections. In ICCV, 2021.\\n[5] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and\\nShelly Sheynin. VideoJAM: Joint appearance-motion representations for enhanced motion generation in\\nvideo models. arXiv: 2502.02492, 2025.\\n[6] Wenzheng Chen, Joey Litalien, Jun Gao, Zian Wang, Clement Fuji Tsang, Sameh Khalis, Or Litany, and\\nSanja Fidler. DIB-R++: Learning to predict lighting and material with a hybrid differentiable renderer. In\\nNeurIPS, 2021.\\n[7] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon\\nVandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using\\nphotogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023.\\n[8] Mohammad Reza Karimi Dastjerdi, Jonathan Eisenmann, Yannick Hold-Geoffroy, and Jean-François\\nLalonde. Everlight: Indoor-outdoor editable hdr lighting estimation. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV), pages 7420–7429, October 2023.\\n[9] Paul E. Debevec and Jitendra Malik. Recovering high dynamic range radiance maps from photographs.\\nIn Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIG-\\nGRAPH ’97, page 369–378, USA, 1997. ACM Press/Addison-Wesley Publishing Co.\\n[10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt,\\nKiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3D objects.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13142–\\n13153, 2023.\\n[11] Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and Anand Bhattad. Generative models: What do\\nthey know? do they know things? let’s find out!, 2024.\\n[12] Egor Ershov, Alexey Savchik, Illya Semenkov, Nikola Bani´c, Alexander Belokopytov, Daria Senshina,\\nKarlo Košˇcevi´c, Marko Subaši´c, and Sven Lonˇcari´c. The cube++ illumination estimation dataset. IEEE\\naccess, 8:227511–227527, 2020.\\n[13] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao\\nLong. GeoWizard: unleashing the diffusion priors for 3D geometry estimation from a single image. In\\nECCV, 2024.\\n[14] Marc-André Gardner, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Christian Gagné, and Jean-François\\nLalonde. Deep parametric indoor lighting estimation. In ICCV, pages 7175–7183, 2019.\\n[15] Marc-André Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen, Emiliano Gambaretto, Christian\\nGagné, and Jean-François Lalonde. Learning to predict indoor illumination from a single image. arXiv\\npreprint arXiv:1704.00090, 2017.\\n[16] Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, and Jean-François Lalonde. Fast spatially-\\nvarying indoor lighting estimation. In CVPR, pages 6908–6917, 2019.\\n[17] Roger Grosse, Micah K. Johnson, Edward H. Adelson, and William T. Freeman. Ground truth dataset and\\nbaseline evaluations for intrinsic image algorithms. In ICCV, pages 2335–2342. IEEE, 2009.\\n[18] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. Shape, light, and material decomposition from\\nimages using Monte Carlo rendering and denoising. arXiv:2206.03380, 2022.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11'}, page_content='[19] Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu,\\nand Ying-Cong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction.\\narXiv preprint arXiv:2409.18124, 2024.\\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\\nInformation Processing Systems, 33:6840–6851, 2020.\\n[21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.\\nVideo diffusion models. arXiv:2204.03458, 2022.\\n[22] Yannick Hold-Geoffroy, Akshaya Athawale, and Jean-François Lalonde. Deep sky modeling for single\\nimage outdoor lighting estimation. In CVPR, pages 6927–6935, 2019.\\n[23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on\\nLearning Representations, 2022.\\n[24] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan.\\nDepthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR, 2025.\\n[25] Wenzel Jakob, Sébastien Speierer, Nicolas Roussel, Merlin Nimier-David, Delio Vicini, Tizian Zeltner,\\nBaptiste Nicolet, Miguel Crespo, Vincent Leroy, and Ziyi Zhang. Mitsuba 3 renderer, 2022. https://mitsuba-\\nrenderer.org.\\n[26] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah\\nSnavely. Neural gaffer: Relighting any object via diffusion. In Advances in Neural Information Processing\\nSystems, 2024.\\n[27] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\\ngenerative models. In Proc. NeurIPS, 2022.\\n[28] Bingxin Ke, Dominik Narnhofer, Shengyu Huang, Lei Ke, Torben Peters, Katerina Fragkiadaki, Anton\\nObukhov, and Konrad Schindler. Video depth without video models. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition (CVPR), 2025.\\n[29] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler.\\nRepurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\\n[30] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for single-view material\\nestimation. In arxiv, 2023.\\n[31] Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel, Jay Busch, and Paul\\nDebevec. Deeplight: Learning illumination for unconstrained mobile mixed reality. In CVPR, pages\\n5918–5928, 2019.\\n[32] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker.\\nInverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from a single\\nimage. In CVPR, pages 2475–2484, 2020.\\n[33] Ruofan Liang, Huiting Chen, Chunlin Li, Fan Chen, Selvakumar Panneer, and Nandita Vijaykumar.\\nEnvidr: Implicit differentiable renderer with neural environment lighting. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 79–89, 2023.\\n[34] Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander\\nKeller, Nandita Vijaykumar, Sanja Fidler, and Zian Wang. Diffusionrenderer: Neural inverse and forward\\nrendering with video diffusion models. arXiv preprint arXiv: 2501.18590, 2025.\\n[35] Ruofan Liang, Zan Gojcic, Merlin Nimier-David, David Acuna, Nandita Vijaykumar, Sanja Fidler, and\\nZian Wang. Photorealistic object insertion with diffusion-guided inverse rendering. In ECCV, 2024.\\n[36] Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao\\nYao, and Shiwei Li. Matrix3D: Large Photogrammetry Model All-in-One, 2025.\\n[37] Linjie Lyu, Ayush Tewari, Marc Habermann, Shunsuke Saito, Michael Zollhöfer, Thomas Leimkühler,\\nand Christian Theobalt. Diffusion posterior illumination for ambiguity-aware inverse rendering. ACM\\nTransactions on Graphics, 42(6), 2023.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12'}, page_content='[38] Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, and\\nBastian Leibe. Fine-tuning image-conditional diffusion models is easier than you think. In Proceedings of\\nthe IEEE/CVF Winter Conference on Applications of Computer Vision (W ACV), 2025.\\n[39] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta.\\nObject 3dit: Language-guided 3d-aware image editing. Advances in Neural Information Processing\\nSystems, 36:3497–3516, 2023.\\n[40] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.\\nNeRF: representing scenes as neural radiance fields for view synthesis. arXiv preprint arXiv:2003.08934,\\n2020.\\n[41] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Müller,\\nand Sanja Fidler. Extracting triangular 3D models, materials, and lighting from images. arXiv:2111.12503,\\n2021.\\n[42] Jacob Munkberg, Zian Wang, Ruofan Liang, Tianchang Shen, and Jon Hasselgren. VideoMat: Extracting\\nPBR Materials from Video Diffusion Models. In Eurographics Symposium on Rendering - CGF Track,\\n2025.\\n[43] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the\\nIEEE/CVF international conference on computer vision, pages 4195–4205, 2023.\\n[44] Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Amit Raj, Varun Jampani,\\nPramook Khungurn, and Supasorn Suwajanakorn. DiffusionLight: light probes for free by painting a\\nchrome ball. In ArXiv, 2023.\\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), 2022.\\n[46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\\nimage diffusion models with deep language understanding. Advances in Neural Information Processing\\nSystems, 35:36479–36494, 2022.\\n[47] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In The\\nTenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,\\n2022. OpenReview.net, 2022.\\n[48] Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, and Jan Kautz. Neural\\ninverse rendering of an indoor scene from a single image. In ICCV, 2019.\\n[49] Shuran Song and Thomas Funkhouser. Neural illumination: Lighting prediction for indoor environments.\\nIn CVPR, pages 6918–6926, 2019.\\n[50] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Alek-\\nsei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust\\nlarge mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on\\napplications of computer vision, pages 2149–2159, 2022.\\n[51] Jiajun Tang, Yongjie Zhu, Haoyu Wang, Jun-Hoong Chan, Si Li, and Boxin Shi. Estimating spatially-\\nvarying lighting in urban scenes with disentangled representation. In ECCV, 2022.\\n[52] Benjamin Ummenhofer, Sanskar Agrawal, Rene Sepulveda, Yixing Lao, Kai Zhang, Tianhang Cheng,\\nStephan Richter, Shenlong Wang, and German Ros. Objects with lighting: A real-world dataset for\\nevaluating reconstruction and rendering for object relighting. In 2024 International Conference on 3D\\nVision (3DV), pages 137–147. IEEE, 2024.\\n[53] Giuseppe Vecchio and Valentin Deschaintre. Matsynth: A modern pbr materials dataset. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\\n[54] Guangcong Wang, Yinuo Yang, Chen Change Loy, and Ziwei Liu. Stylelight: Hdr panorama generation\\nfor lighting estimation and editing. In European Conference on Computer Vision (ECCV), 2022.\\n[55] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning\\nneural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS, 2021.\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13'}, page_content='[56] Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, and Jian Zhang. 360dvd: Controllable panorama\\nvideo generation with 360-degree video diffusion model. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, pages 6913–6923, 2024.\\n[57] Zian Wang, Wenzheng Chen, David Acuna, Jan Kautz, and Sanja Fidler. Neural light field estimation for\\nstreet scenes with differentiable virtual object insertion. In ECCV, 2022.\\n[58] Zian Wang, Jonah Philion, Sanja Fidler, and Jan Kautz. Learning indoor inverse rendering with 3D\\nspatially-varying lighting. In ICCV, 2021.\\n[59] Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob Munkberg, Jon Hasselgren, Zan Gojcic,\\nWenzheng Chen, and Sanja Fidler. Neural fields meet explicit geometric representations for inverse\\nrendering of urban scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\\nJune 2023.\\n[60] Henrique Weber, Mathieu Garon, and Jean-François Lalonde. Editable indoor lighting estimation. In\\nEuropean Conference on Computer Vision, pages 677–692. Springer, 2022.\\n[61] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and\\nChunhua Shen. What matters when repurposing diffusion models for general dense perception tasks?\\narXiv preprint arXiv:2403.06090, 2024.\\n[62] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi\\nHong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert\\ntransformer. arXiv preprint arXiv:2408.06072, 2024.\\n[63] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang\\nXiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM\\nTransactions on Graphics, 2024.\\n[64] Hong-Xing Yu, Samir Agarwala, Charles Herrmann, Richard Szeliski, Noah Snavely, Jiajun Wu, and\\nDeqing Sun. Accidental light probes. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pages 12521–12530, 2023.\\n[65] Ye Yu and William A. P. Smith. InverseRenderNet: learning single image inverse rendering. In CVPR,\\n2019.\\n[66] Greg Zaal and et al. Poly Haven - The Public 3D Asset Library, 2025.\\n[67] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-\\nQi Yan, and Miloš Hašan. RGB↔X: image decomposition and synthesis using material-and lighting-aware\\ndiffusion models. In ACM SIGGRAPH 2024 Conference Papers, pages 1–11, 2024.\\n[68] Fangneng Zhan, Changgong Zhang, Yingchen Yu, Yuan Chang, Shijian Lu, Feiying Ma, and Xuansong\\nXie. Emlight: Lighting estimation via spherical distribution approximation. In Proceedings of the AAAI\\nConference on Artificial Intelligence, 2021.\\n[69] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. PhySG: Inverse rendering with\\nspherical Gaussians for physics-based material editing and relighting. In CVPR, 2021.\\n[70] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect\\nillumination for inverse rendering. In CVPR, 2022.\\n[71] Yiqin Zhao and Tian Guo. Pointar: Efficient lighting estimation for mobile augmented reality. arXiv\\npreprint arXiv:2004.00006, 2020.\\n[72] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient\\nvideo generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.\\n[73] Yongjie Zhu, Yinda Zhang, Si Li, and Boxin Shi. Spatially-varying outdoor lighting estimation from\\nintrinsics. In CVPR, 2021.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14'}, page_content='Supplement for LuxDiT: Lighting Estimation with\\nVideo Diffusion Transformer\\nIn the supplementary material, we discuss the broader impact of our project in Sec. A, and provide\\nadditional details for implementation and experiments in Sec. B. Sec. C provides additional quantita-\\ntive and qualitative results. We refer to the accompanied video for extended comparisons on video\\nlighting estimation.\\nA Broader Impact\\nWe introduce LuxDiT, a generative model for estimating high-dynamic-range (HDR) environment\\nlighting from casually captured images and videos. Lighting estimation is a core challenge in\\nphotorealistic rendering due to its non-local and indirect nature. LuxDiT produces scene-consistent\\nHDR panoramas, enabling applications in virtual object insertion, relighting, AR/VR, and visual\\neffects. It can also support synthetic data generation for downstream tasks in robotics and perception,\\nwhere realistic illumination is critical.\\nSimilar to other generative methods, LuxDiT could be misused to produce visually convincing\\nbut deceptive content. While it does not directly generate synthetic scenes, it enables realistic\\nvirtual object insertion and may facilitate the creation of manipulated imagery that is difficult to\\ndistinguish from real footage. We encourage responsible use of LuxDiT and caution against its\\ndeployment in contexts where synthetic content could mislead viewers or undermine public trust,\\nsuch as misinformation or falsified media.\\nB Additional Details\\nB.1 HDR Reconstruction\\nSection 4.1 describes our method for reconstructing HDR environment maps from two tone-mapped\\nLDR images using a lightweight MLP ψ (Eldr, Elog). This MLP consists of 5 layers with 64 hidden\\nunits per layer and LeakyReLU activation. A softplus activation is applied to the final output layer to\\nensure non-negative outputs.\\nThe MLP ψ operates on a per-pixel basis: it takes a pair of LDR RGB values as input and predicts a\\nsingle HDR RGB value. It is trained using the same HDR environment maps as the diffusion model,\\nwith augmentations including random intensity rescaling and exposure adjustments for diversity. To\\nsimulate limited input precision, LDR inputs are randomly quantized to 8-bit RGB values. We train\\nthe MLP using a Huber loss with δ = 1.0, which provides robustness against large HDR outliers\\nwhile preserving smooth gradients.\\nAdditionally, we show the tone-mapping curves used to generate the LDR images in Fig. 6. Our\\ndual-tone mapping strategy ensures sufficient sampling across the full dynamic range [0, 10,000],\\nsupporting accurate HDR reconstruction.\\n0 1 4 8 16 100 1000 10000\\nHDR Value\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0LDR Value\\nReinhard\\nLogarithm\\nFigure 6: The two tone-mapping curves used to gen-\\nerate the LDR images. The 128 dot points along the\\ncurve are evenly spaced along [0, 1] LDR value range.\\n0.04 2.93 (50%) 837.67 (max)\\n99.9-th Percentile Intensity (Log Scale)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160Frequency\\nFigure 7: The histogram of the 99.9-th percentile\\nintensity of all HDR environment maps in our training\\nset.\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15'}, page_content='B.2 Datasets\\nWe provide more details about the datasets used in our experiments.\\nThe data sources of HDR environment maps.We collected 2386 HDR environment maps from\\nthe following 4 data sources either publicly available or commercially available.\\n• Poly Haven0: 626 HDR environment maps with a wide range of indoor and outdoor lighting.\\n• HDR Maps1: 403 HDR environment maps with diverse lighting conditions, including 294\\npanorama maps and 109 hemi-sphere sky maps.\\n• HDRI Skies2: 457 HDR environment maps with outdoor lighting conditions.\\n• DOSCH DESIGN3: 900 HDR environment maps mainly for outdoor lighting conditions.\\nFigure 7 shows the histogram of the 99.9-th percentile intensity of all HDR environment maps in\\nour training set. With over 50% of the HDR environment maps having a 99.9-th percentile intensity\\ngreater than 2.93. Note that the for outdoor lighting, the highest intensity can be orders of magnitude\\nhigher than the 99.9-th percentile. Among these, Poly Haven and HDR Maps offer greater diversity\\nacross scene types. To balance the training distribution across data sources, we apply sampling\\nweights in the ratio 3:2:2:1 in the order listed above.\\nFor quantitative and qualitative evaluation, we use the Laval Indoor4 and Laval Outdoor 5 datasets,\\nwhich contain calibrated HDR panoramas of real-world indoor and outdoor scenes.\\nFigure 8: Randomly sampled example images\\nfrom our synthetic rendering data.\\nSynthetic rendering data.Similar to OBJect [39]\\nand DiffusionRenderer [34], we create synthetic 3D\\nscenes by compositing multiple 3D objects from Ob-\\njaverse [10] and randomly placing them on a plane\\nwith varying plane textures. We use a filtered sub-\\nset of Objaverse, containing ∼269,000 3D objects\\nwith decent geometries and material textures, to cre-\\nate synthetic 3D scenes. The varying plane textures\\nare sampled from ∼4000 PBR textures from Mat-\\nSynth6 [53]. Each composited scene contains up to\\n3 sampled Objaverse objects. We additionally add up\\nto 3 random geometry primitives (sphere, cube, and\\ncylinder) with varying material textures to provide\\nrich shading cues for model to learn. For each scene,\\nwe randomly render 1 ∼4 video clips with varying\\ncamera motions (e.g., orbiting camera and oscilating\\ncamera) and environment lightings. We use a path-tracing renderer with 128 samples per pixel (spp)\\nand the default OptiX denoiser to render the video clips with a resolution of 480 × 720 or 512 × 512.\\nThe HDR rendering results are tone-mapped to LDR images using Blender’s AgX tonemapping7. In\\ntotal, we created ∼190,000 random synthetic scenes, resulting in ∼260,000 video clips with at least\\n16 frames per video clip.\\nPerspective crops of HDR panorama images.We use a subset of 1251 HDR panoramas with\\nmeaningful contents from Poly Haven, HDR Maps, and HDRI Skies for the training with perspective\\ncrops. Instead of pre-processing the perspective crops from the HDR panoramas, we do the perspective\\ncrops on-the-fly during the training. The projection camera’s azimuth angle is randomly sampled\\nfrom [0, 360◦] and the elevation angle is randomly sampled from −10◦ to 10◦. The camera’s field\\nof view (FOV) is randomly sampled from 45◦ to 80◦. The perspective crops are rendered with a\\nresolution of 480 × 720. A random tone-mapping function is applied to perspective projection crops\\n0https://polyhaven.com/\\n1https://hdrmaps.com/\\n2https://hdri-skies.com/\\n3https://doschdesign.com/\\n4http://hdrdb.com/indoor/\\n5http://hdrdb.com/outdoor/\\n6https://huggingface.co/datasets/gvecchio/MatSynth\\n7https://www.blender.org/\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 15, 'page_label': '16'}, page_content='to generate LDR images. The tone-mappings include ACES, Filmic, AgX, and Gamma-2.4 sRGB\\nmappings. Auto-exposure (i.e., remapping the 99-th percentile intensity to 0.9) is also randomly\\napplied to the LDR crops. For video input, we create trajectories of projection cameras by smoothly\\nrotating the camera angle within an angular cone of 15◦.\\nPerspective crops of LDR panorama videos.Similar to the perspective crops of HDR panorama\\nimages. We on-the-fly sample perspective crops from the LDR panorama videos. Due to the lack of\\nHDR content, we only apply a random auto-exposure tone-mapping to the perspective crops.\\nB.3 Model Details and Initialization\\nLuxDiT is fine-tuned from the pre-trained CogVideoX-5b-I2V8. To adapt this model for our task,\\nwe replace the original text token with an image input token. This image token is generated in the\\nsame manner as the environment map noise token, but without adding noise. We reuse the model’s\\nexisting text-processing layers (e.g., AdaLN) to process these new image input tokens. Furthermore,\\nwe extend the input projection layer to incorporate additional conditioning channels derived from\\nthe concatenated noise token; these extended channels are initialized to zero. Similarly, the output\\nprojection layer is extended to predict dual tone-mapped environment tokens, with its newly added\\nchannels initialized from the original model’s weights.\\nB.4 User Study Details for Virtual Object Insertion\\nFollowing prior works [14, 16, 15, 57, 35], we conduct a user study on Amazon Mechanical Turk to\\ncompare our method against baseline approaches in terms of perceptual realism for virtual object\\ninsertion. Each participant is shown a pair of rendered results—one from our method and one from a\\nbaseline—and asked to assess lighting realism, focusing on shadows, reflections, and overall visual\\nintegration.\\nThe specific instructions shown to participants are:\\nInstruction: Find the inserted virtual object, look at the difference, and select the\\nmore realistic image.\\nAn AI system is trying to insert a virtual object into an image in a natural way.\\nIt aims to make the virtual object look as if it is part of the scene. There are two\\nresults: Trial A and Trial B, and the virtual object is located in the center of each\\nimage. Please zoom in to compare the differences between the two images, and\\npay attention to the lighting effects such as the reflections and shadows.\\nWhich one looks more realistic?\\n□ A\\n□ B\\nParticipants are required to use a monitor 24 inches or larger. Image pairs are randomly shuffled to\\nprevent bias. Following [35], we repeat the user study three times, and recruited 11 unique participants\\nfor each experiment. We compute the percentage of images for which users preferred our method\\nover the baseline, and report the average user preferences for three repeated experiments. In total, the\\nstudy includes 11 × 3 × 11 × 3 = 1089 individual comparisons.\\nB.5 Three-sphere Evaluation Protocol\\nWe adopt the three-sphere rendering setting described in StyleLight [ 54], with evaluation scripts\\nprovided by DiffusionLight9.\\nFor the Laval Indoor dataset, we use the same set of HDR environment maps and corresponding\\nperspective crops as DiffusionLight. We resize and crop the input image to 480 × 720 for our model.\\nFor Laval Outdoor and Poly Haven environment maps, we generate perspective crops using a fixed\\nhorizontal camera with a 60◦ field of view and a resolution of 480 × 720. For Laval Outdoor, we\\napply auto-exposure by scaling the 50th percentile intensity to 0.5.\\n8https://huggingface.co/THUDM/CogVideoX-5b-I2V\\n9https://github.com/DiffusionLight/DiffusionLight-evaluation\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17'}, page_content='C Additional Experiments\\nC.1 Array-of-Spheres Evaluation\\nTable 9: Scores on indoor array-of-spheres.\\nMethod si-RMSE ↓ AE ↓\\nEverLight [8] 0.091 6.36\\nStyleLight [54] 0.123 7.09\\nWeber et al. [60] 0.081 4.13\\nEMLight [68] 0.099 3.99\\nDiffusionLight [44] 0.090 5.25\\nOurs 0.089 4.90\\nFollowing prior work [ 60, 8], we evaluate our method\\nusing the array-of-spheres protocol, which renders a grid\\nof diffuse spheres on a ground plane using the predicted\\nenvironment map.\\nWe use 2,240 perspective crops from 224 Laval Indoor\\npanoramas, provided by DiffusionLight10. All input im-\\nages are resized to 512 × 512 to match our model input.\\nQuantitative results are shown in Table 9 and qualitative\\nresults in Fig. 9.\\nWhile our method performs slightly below specialized sys-\\ntems like Weber et al. [60] and EMLight [68], it remains\\ncompetitive—despite not being trained on Laval Indoor.\\nNotably, it outperforms StyleLight [54] and DiffusionLight [44], demonstrating strong generalization\\nacross lighting domains.\\nInput Image GT DiffusionLight Ours Input Image GT DiffusionLight Ours\\nFigure 9: Visual results on array-of-spheres protocol.\\nC.2 Lighting Estimation with the Cube++ Dataset\\nTable 10: Scores on SpyderCube white face\\nrendering on Cube++ dataset.\\nMethod RMSE ↓ AE ↓\\nLeft Right Left Right\\nD.Light [44] 0.044 0.035 7.221 5.741\\nOurs 0.024 0.025 3.985 4.003\\nWe also evaluated our method on the Cube++\\ndataset [12], specifically designed for illumination esti-\\nmation and color constancy. This dataset includes illu-\\nmination information annotated by the SpyderCube 11.\\nFor our experiment, we selected 100 processed JPEG\\nimages from Cube++. We then applied both Diffusion-\\nLight and our method to estimate the illumination from\\neach image. Subsequently, we rendered the left and\\nright white faces of the SpyderCube under the estimated\\nillumination, assuming purely Lambertian diffuse sur-\\nfaces. To prevent information leakage from the SpyderCube in the input images, we masked out\\nthe SpyderCube from the tested images and inpainted the masked region using LaMa [50]. We then\\ncompared the rendered face colors to the colors sampled directly from the SpyderCube JPEG images.\\nTable 10 presents the RMSE and angular errors, demonstrating that our method clearly outperforms\\nDiffusionLight, achieving angular errors of less than 5◦ on both faces. Visual comparison results are\\nfurther illustrated in Fig. 10.\\n10https://github.com/DiffusionLight/image-array_of_spheres\\n11https://www.datacolor.com/spyder/products/spyder-cube/\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 17, 'page_label': '18'}, page_content='GT Ours D.Light GT Ours D.Light GT Ours D.Light GT Ours D.Light GT Ours D.Light\\nFigure 10: Visual results on Cube++ dataset. We show the rendered two white cube faces, mirror ball, and\\nmatte silver ball from our method and DiffusionLight for visual comparison.\\nC.3 Lighting Estimation from Foreground Objects\\nSince our model is trained on object-centric synthetic rendering data, we can also apply it to estimate\\nlighting from foreground objects. We selected 4 NeRF synthetic objects [ 40] and 4 real-world\\nobjects [52], aiming to estimate lighting from videos containing nine consecutive rendering views.\\nWe qualitatively compare LuxDiT with optimization-based inverse rendering methods [41, 18] that\\nreconstruct 3D geometry and lighting from full NeRF scenes. Using the ground truth camera poses,\\nwe rotate each frame’s estimated lighting into the global coordinate system and average across frames\\nto produce the final environment map.\\nQualitative results are shown in Fig.11. On mostly diffuse objects like lego and hotdog, our method\\nrecovers highlight directions accurately, enabling shadow rendering consistent with the input. For\\nglossy objects like mic and ficus, our model estimates lighting nearly identical to the ground truth.\\nWhile these HDR environment maps are included in our training set, the NeRF scenes differ signifi-\\ncantly from our synthetic renderings (see Fig. 8), indicating that our model leverages shading cues\\nand learned priors rather than direct memorization. In contrast, optimization-based baselines struggle\\nto capture high-frequency lighting detail and often introduce noise and artifacts in lighting.\\nWe further tested our method on real-world foreground objects from the Objects-with-Lighting\\ndataset [52], which provides ground truth distant environment lighting. Similar to the NeRF synthetic\\nscene setup, the estimated lighting was then aligned into the global coordinate system using ground\\ntruth camera poses. We compared our approach to NeuS+Mitsuba [ 55, 25], the top-performing\\nmethod on this dataset [52]. The metrics, using the three-sphere protocol, are presented in Table 11,\\nwith visual results in Fig. 12.\\nWhile our model performs well overall, minor errors remain, e.g. color shifts in the NeRF Lego\\nscene (Fig. 11) and a slightly higher si-RMSE compared to NeuS+Mitsuba (Table 11). We believe\\ncombining our generative model with optimization-based methods could further enhance lighting\\nestimation, which we leave for future work.\\nTable 11: Comparison of our method with NeuS+Mitsuba on Objects with Lighting datasets.\\nMethod Scale-invariant RMSE↓ Angular Error↓ Normalized RMSE↓\\nDiffuse Matte Mirror Diffuse Matte Mirror Diffuse Matte Mirror\\nNeuS+Mitsuba 0.082 0.232 0.424 3.145 3.383 3.526 0.180 0.545 0.717\\nOurs 0.086 0.253 0.482 1.262 1.594 2.000 0.153 0.339 0.479\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 18, 'page_label': '19'}, page_content='GT & Input NVDIFFREC [41] NVDIFFRECMC [18] Ours\\nFigure 11: Lighting estimation from the NeRF synthetic objects. We use the estimated lighting from different\\nmethods to re-render the original NeRF Blender scenes.\\nOurs NeuS+Mitsuba\\nInput GT\\nOurs NeuS+Mitsuba\\nInput GT\\n Input GT\\nOurs NeuS+Mitsuba\\nFigure 12: Lighting estimation from the masked real objects from Objects with Lighting.\\nC.4 Additional Ablations\\nC.4.1 The Choice of the HDR Fusion Model\\nAs detailed in Sec. 4.1, a lightweight MLP ψ is employed to merge the dual-tonemapped environment\\nmaps, Eldr and Elog, thereby reconstructing the HDR environment map ˆE. There are also alternative\\nfusion methods, such as using a more complex CNN model to incorporate adjacent pixel information\\nfor HDR fusion, or applying a rule-based approach with explicit inverse equations. To justify our\\nchoice of a simple MLP, we evaluate various HDR fusion techniques, including MLP, CNN, and a\\nrule-based method. The CNN model has an identical number of layers to our MLP model, using3 ×3\\nconvolution kernels across layers. The rule-based method involves applying the inverse Reinhard map\\nfor lights with intensity below 8, a linear interpolation between Reinhard and log maps for intensities\\nranging from 8 to 16, and exclusively the log map for intensities exceeding 16.\\nTable 12: Comparison on differ-\\nent HDR fusion approaches.\\nMLP CNN Rule\\nRMSE↓ 11.55 11.74 11.71\\nTable 12 presents the RMSE results on testing Polyhaven HDRIs.\\nAll three methods demonstrate comparable accuracy, with the MLP\\napproach exhibiting a slight advantage. Compared to the rule-based\\napproach, we believe the neural approach can better handle numer-\\nical inconsistency after image uint8 quantization, and the potential\\ndata range overflow (e.g., lights beyond the pre-defined maximum\\nintensity 10000).\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20'}, page_content='C.4.2 The Impact of LoRA on Synthetic Scenes\\nSection 5.5 demonstrates the impact of varying LoRA scales (0.0 to 1.0) on the predicted lighting\\ncontent of real-world images. This ablation study, conversely, investigates how our LoRA model,\\ntrained with real images, affects the lighting estimation of synthetic foreground objects. Table 13\\npresents the angular errors using a three-sphere evaluation, and Fig. 16 provides the visual results.\\nTable 13: Ablation study on impact of LoRA scale on synthetic foreground objects.\\nLoRA Scale 0.00 0.25 0.50 0.75 1.00\\nDiffuse ↓ 1.594 1.737 2.170 3.832 3.937\\nMatte ↓ 2.068 2.311 2.914 5.322 5.891\\nMirror ↓ 3.405 3.690 4.342 6.783 7.400\\nIn contrast to the ablation performed on scene images, a larger LoRA scale leads to lower lighting\\nestimation accuracy. As Fig. 16 illustrates, increasing the LoRA scale causes foreground content\\nto gradually appear on the estimated environment map, which is consistent with our LoRA model’s\\nbehavior. Nevertheless, the estimated highlights remain consistent across different LoRA scales.\\nD Additional Results\\nWe provide additional visual results in this section to further support the claims made in the main\\npaper.\\n• Model Ablation and LoRA Scale: Figure 13 details the ablation study on our model’s design\\nand the exploration of different LoRA scales.\\n• Camera Parameter Variations: Figures 14 and 15 show lighting estimation performance\\nwhen varying camera field of view (FOV) and elevation angles, respectively.\\n• Three-Sphere Rendering Evaluations: Figures 17, 18, and 19 display further lighting estima-\\ntion outcomes using the three-sphere rendering protocol on the Laval Indoor, Laval Outdoor,\\nand Poly Haven datasets.\\n• Virtual Object Insertion: Figures 20 and 21 illustrate additional virtual object insertion\\nresults on Poly Haven panorama crops and Waymo driving scenes.\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 20, 'page_label': '21'}, page_content='Input Image Model Ablation LoRA Scale Exploration\\nchannel concat. LoRA 0.0 LoRA 0.1 LoRA 0.2 LoRA 0.4\\nw/o syn. data LoRA 0.6 LoRA 0.7 LoRA 0.8 LoRA 1.0\\nFigure 13: Model design ablation and LoRA scale exploration. The “Model Ablation” column\\nshows the results of our two model design variants: 1) channel concatenation and 2) training without\\nsynthetic rendering data. The “LoRA Scale Exploration” columns show the visual results of our\\nmodel with different LoRA scales.\\nFOV 45◦ 50◦ 60◦ 70◦ 75◦\\nGT\\nFigure 14: Lighting estimation from input images with varying camera FOV .\\nElevation 30◦ ↑ 15◦ ↑ 0◦ 15◦ ↓ 30◦ ↓\\nGT\\nFigure 15: Lighting estimation from input images with varying camera elevation.\\nInput Image LoRA Scale Exploration\\nGT LoRA 0.00 LoRA 0.25\\nLoRA 0.50 LoRA 0.75 LoRA 1.00\\nFigure 16: LoRA scale exploration on synthetic foreground scenes.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22'}, page_content='Input Image GT StyleLight DiffusionLight Ours\\nFigure 17: Additional qualitative results on Laval Indoor dataset.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23'}, page_content='Input Image GT H-G et al. NLFE DiffusionLight Ours\\nFigure 18: Additional qualitative results on Laval Outdoor dataset.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 23, 'page_label': '24'}, page_content='Input Image GT StyleLight NLFE DiffusionLight Ours\\nFigure 19: Additional qualitative results on Poly Haven dataset.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 24, 'page_label': '25'}, page_content='Reference StyleLight DiffusionLight DiPIR Ours\\nFigure 20: Additional virtual object insertion on Poly Haven perspective crops.\\nH-G et al. [22] NLFE DiffusionLight DiPIR Ours\\nFigure 21: Additional virtual object insertion on Waymo driving scenes.\\n25')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = web_data + pdf_data\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cffc8d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LuxDiT: Lighting Estimation with Video Diffusion\n",
      "Transformer\n",
      "Ruofan Liang1,2,3 Kai He1,2,3 Zan Gojcic1 Igor Gilitschenski2,3\n",
      "Sanja Fidler1,2,3 Nandita Vijaykumar2,3† Zian Wang1,2,3†\n",
      "1NVIDIA 2University of Toronto 3Vector Institute\n",
      "Abstract\n",
      "Estimating scene lighting from a single image or video remains a longstand-\n",
      "ing challenge in computer vision and graphics. Learning-based approaches are\n",
      "constrained by the scarcity of ground-truth HDR environment maps, which are\n",
      "expensive to capture and limited in diversity. While recent generative models\n",
      "offer strong priors for image synthesis, lighting estimation remains difficult due\n",
      "to its reliance on indirect visual cues, the need to infer global (non-local) con-\n",
      "text, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a\n",
      "novel data-driven approach that fine-tunes a video diffusion transformer to gen-\n",
      "erate HDR environment maps conditioned on visual input. Trained on a large\n",
      "synthetic dataset with diverse lighting conditions, our model learns to infer il-\n",
      "lumination from indirect visual cues and generalizes effectively to real-world\n",
      "scenes. To improve semantic alignment between the input and the predicted en-\n",
      "vironment map, we introduce a low-rank adaptation finetuning strategy using a\n",
      "collected dataset of HDR panoramas. Our method produces accurate lighting\n",
      "predictions with realistic angular high-frequency details, outperforming existing\n",
      "state-of-the-art techniques in both quantitative and qualitative evaluations. Project\n",
      "page: https://research.nvidia.com/labs/toronto-ai/LuxDiT/\n",
      "1 Introduction\n",
      "In physically-based rendering, lighting plays a central role in shaping the appearance—how objects\n",
      "cast shadows, reflect, and appear integrated within a scene. From virtual object insertion and\n",
      "augmented reality to synthetic data generation, many downstream tasks rely on estimating scene\n",
      "illumination. Yet inferring lighting from casually captured images or video remains an open challenge.\n",
      "A common representation of the scene illumination is the high-dynamic-range (HDR) environment\n",
      "map, which describes incoming light intensity from all directions. HDR maps can be acquired by\n",
      "using light probes or multi-exposure panoramas, requiring specialized setups that are impractical for\n",
      "everyday use [9]. To overcome this, several learning-based methods that estimate environment maps\n",
      "directly from casually captured LDR images or videos have been proposed [15, 16, 32, 73]. However,\n",
      "these methods typically depend on paired datasets of input images or videos and HDR environment\n",
      "maps, leading to a chicken-and-egg problem: a large collection of HDR environment maps is needed\n",
      "to train a model that aims to alleviate the need for acquiring such expensive data in the first place.\n",
      "Recently, generative diffusion models have demonstrated strong capabilities in modeling complex\n",
      "image distributions. DiffusionLight [44] demonstrated that pretrained text-to-image models encode\n",
      "implicit knowledge of illumination, which can be cleverly extracted by inpainting a virtual chrome\n",
      "ball into an image, generating plausible appearances under varying exposure settings. However,\n",
      "without task-specific fine-tuning, the inpainting priors of pre-trained diffusion models are insufficient\n",
      "† Joint Advising\n",
      "Preprint.\n",
      "arXiv:2509.03680v1  [cs.GR]  3 Sep 2025\n"
     ]
    }
   ],
   "source": [
    "print(all_data[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183e2319",
   "metadata": {},
   "source": [
    "## Splitting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2bfa805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a6ce900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Documents :  26\n",
      "Total Split :  160\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200,  add_start_index=True)\n",
    "all_splits = text_splitter.split_documents(all_data)\n",
    "print(\"Total Documents : \", len(all_data))\n",
    "print(\"Total Split : \", len(all_splits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4428d03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 971}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 3549}, page_content='Self-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 4312}, page_content='Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 5209}, page_content='Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 6069}, page_content='Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 7030}, page_content='To avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 7445}, page_content='After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8436}, page_content='The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 9172}, page_content='In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 9720}, page_content='Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 10685}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 11585}, page_content='Categorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 12039}, page_content='Maximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 12583}, page_content='LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 13344}, page_content='HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 14117}, page_content='FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 14939}, page_content='Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 15618}, page_content='MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 16535}, page_content='Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17352}, page_content='Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17734}, page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 18591}, page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19303}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\"),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19944}, page_content='(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 20401}, page_content='API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 21014}, page_content='Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 21547}, page_content='This benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 22228}, page_content='Case Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 22956}, page_content='One interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 23929}, page_content='inquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 24126}, page_content='They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 25125}, page_content='Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 25955}, page_content=\"Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\"),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 26585}, page_content='The generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 27526}, page_content='GOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 28069}, page_content='Commands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 28840}, page_content='11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29546}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 30104}, page_content='You should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 30868}, page_content='Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 31348}, page_content='},\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 31986}, page_content='},\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 32858}, page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 32987}, page_content='You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 33804}, page_content='FILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 34657}, page_content='You always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 34969}, page_content='pytest\\ndataclasses'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 34990}, page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 35043}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 35837}, page_content='for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 36645}, page_content='Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 37124}, page_content='},\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 37831}, page_content='\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 38621}, page_content='are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39002}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39137}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39948}, page_content='Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 40518}, page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 41337}, page_content='[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 42106}, page_content='[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'start_index': 0}, page_content='LuxDiT: Lighting Estimation with Video Diffusion\\nTransformer\\nRuofan Liang1,2,3 Kai He1,2,3 Zan Gojcic1 Igor Gilitschenski2,3\\nSanja Fidler1,2,3 Nandita Vijaykumar2,3† Zian Wang1,2,3†\\n1NVIDIA 2University of Toronto 3Vector Institute\\nAbstract\\nEstimating scene lighting from a single image or video remains a longstand-\\ning challenge in computer vision and graphics. Learning-based approaches are\\nconstrained by the scarcity of ground-truth HDR environment maps, which are\\nexpensive to capture and limited in diversity. While recent generative models\\noffer strong priors for image synthesis, lighting estimation remains difficult due\\nto its reliance on indirect visual cues, the need to infer global (non-local) con-\\ntext, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a\\nnovel data-driven approach that fine-tunes a video diffusion transformer to gen-\\nerate HDR environment maps conditioned on visual input. Trained on a large'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'start_index': 788}, page_content='novel data-driven approach that fine-tunes a video diffusion transformer to gen-\\nerate HDR environment maps conditioned on visual input. Trained on a large\\nsynthetic dataset with diverse lighting conditions, our model learns to infer il-\\nlumination from indirect visual cues and generalizes effectively to real-world\\nscenes. To improve semantic alignment between the input and the predicted en-\\nvironment map, we introduce a low-rank adaptation finetuning strategy using a\\ncollected dataset of HDR panoramas. Our method produces accurate lighting\\npredictions with realistic angular high-frequency details, outperforming existing\\nstate-of-the-art techniques in both quantitative and qualitative evaluations. Project\\npage: https://research.nvidia.com/labs/toronto-ai/LuxDiT/\\n1 Introduction\\nIn physically-based rendering, lighting plays a central role in shaping the appearance—how objects\\ncast shadows, reflect, and appear integrated within a scene. From virtual object insertion and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'start_index': 1576}, page_content='In physically-based rendering, lighting plays a central role in shaping the appearance—how objects\\ncast shadows, reflect, and appear integrated within a scene. From virtual object insertion and\\naugmented reality to synthetic data generation, many downstream tasks rely on estimating scene\\nillumination. Yet inferring lighting from casually captured images or video remains an open challenge.\\nA common representation of the scene illumination is the high-dynamic-range (HDR) environment\\nmap, which describes incoming light intensity from all directions. HDR maps can be acquired by\\nusing light probes or multi-exposure panoramas, requiring specialized setups that are impractical for\\neveryday use [9]. To overcome this, several learning-based methods that estimate environment maps\\ndirectly from casually captured LDR images or videos have been proposed [15, 16, 32, 73]. However,\\nthese methods typically depend on paired datasets of input images or videos and HDR environment'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'start_index': 2357}, page_content='directly from casually captured LDR images or videos have been proposed [15, 16, 32, 73]. However,\\nthese methods typically depend on paired datasets of input images or videos and HDR environment\\nmaps, leading to a chicken-and-egg problem: a large collection of HDR environment maps is needed\\nto train a model that aims to alleviate the need for acquiring such expensive data in the first place.\\nRecently, generative diffusion models have demonstrated strong capabilities in modeling complex\\nimage distributions. DiffusionLight [44] demonstrated that pretrained text-to-image models encode\\nimplicit knowledge of illumination, which can be cleverly extracted by inpainting a virtual chrome\\nball into an image, generating plausible appearances under varying exposure settings. However,\\nwithout task-specific fine-tuning, the inpainting priors of pre-trained diffusion models are insufficient\\n† Joint Advising\\nPreprint.\\narXiv:2509.03680v1  [cs.GR]  3 Sep 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'start_index': 0}, page_content='Input Image Lighting Estimation Virtual Object Insertion\\nFigure 1: LuxDiT is a generative lighting estimation model that predicts high-quality HDR envi-\\nronment maps from visual input. It produces accurate lighting while preserving scene semantics,\\nenabling realistic virtual object insertion under diverse conditions.\\nfor producing reliable lighting estimates in a single inference and cannot directly generate HDR\\noutputs. As a result, DiffusionLight relies on an expensive test-time ensemble strategy to improve\\nrobustness. Moreover, sampling multiple exposures through separate inference passes introduces\\ninconsistencies and limits the dynamic range of the reconstructed illumination.\\nIn this work, we formulate lighting estimation as a conditional generative task and propose LuxDiT,\\na neural lighting predictor trained on synthetic data and adapted to real-world scenes. Conditioned on\\nvisual input, our approach fine-tunes a diffusion transformer (DiT) to synthesize HDR panoramas'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'start_index': 790}, page_content='a neural lighting predictor trained on synthetic data and adapted to real-world scenes. Conditioned on\\nvisual input, our approach fine-tunes a diffusion transformer (DiT) to synthesize HDR panoramas\\nfrom noise. Unlike pixel-aligned tasks, lighting estimation requires global reasoning over scene\\ncontext. DiTs are particularly suited to this task: their attention-based architecture supports global\\ncontext aggregation, and their generative priors facilitate reasoning from indirect cues such as shading\\nand reflections.\\nTraining such a model requires diverse lighting data. To overcome the lack of real-world HDR lighting\\nsupervision, we construct a large-scale synthetic dataset with randomized geometry, materials, and\\nlighting conditions. Training on this dataset allows the model to learn physically grounded cues for\\nlight direction and intensity. While this imparts general lighting priors, models trained purely on'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'start_index': 1613}, page_content='light direction and intensity. While this imparts general lighting priors, models trained purely on\\nsynthetic data often hallucinate lighting based on dataset priors, producing environment maps that\\nare plausible but semantically mismatched with the input scene. For example, an image of an urban\\nstreet may yield an environment map depicting a rural landscape. To address this, we further apply\\nlow-rank adaptation (LoRA) [23] on a curated set of real HDR panoramas, improving alignment\\nbetween predicted lighting and scene semantics.\\nGiven a single image or video, LuxDiT produces HDR environment maps with accurate direction,\\nintensity, and scene-consistent content. It reduces lighting estimation error by 45% on Laval Outdoor\\nsunlight direction and improves temporal consistency for video input, enabling reliable use in\\ndownstream applications such as virtual object insertion. Our main contributions are:'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'start_index': 2344}, page_content='sunlight direction and improves temporal consistency for video input, enabling reliable use in\\ndownstream applications such as virtual object insertion. Our main contributions are:\\n• A DiT-based generative architecture that synthesizes HDR environment maps from visual input.\\n• A LoRA-based fine-tuning strategy using curated HDR panoramas to improve semantic alignment\\nbetween the input scene and predicted illumination.\\n• A large-scale synthetic dataset with randomized geometry, materials, and lighting.\\n2 Related Work\\nLighting estimation aims to infer environment illumination from input imagery, and is critical for\\nphotorealistic rendering and virtual object insertion. Early learning-based methods treat lighting\\nestimation as a supervised regression problem, predicting spherical lobes [16, 32, 71, 68], parametric\\nsources [65, 14], or low-resolution environment maps [15, 49, 73, 51] directly from a single image.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'start_index': 3167}, page_content='sources [65, 14], or low-resolution environment maps [15, 49, 73, 51] directly from a single image.\\nThese models are trained on paired data obtained from real-world captures [15, 49, 57] or synthetic\\nrendering [32, 73, 51]. However, their performance often degrades in complex, in-the-wild scenes\\ndue to limited diversity in the training data.\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'start_index': 0}, page_content='Recent methods incorporate generative priors to address the ambiguity of scene illumination. Style-\\nLight [54] fine-tunes a StyleGAN to generate LDR and HDR panoramas from latent codes, using\\nGAN inversion at test time. However, its performance hinges on inversion quality and often breaks\\nsemantic alignment on out-of-domain inputs. EverLight [8] regresses a parametric lighting estimate\\nand refines it with a GAN to add high-frequency detail, but relies on pseudo-labeled HDR data and\\nstruggles with complex or bright lighting. DiffusionLight [ 44] uses a diffusion model to inpaint\\na virtual chrome ball under multiple exposures, merging them into an HDR map. While visually\\nplausible, this multi-stage process yields distorted panoramas and limited dynamic range.\\nInverse renderingrecovers scene properties such as geometry, material reflectance, and illumination\\nfrom image observations. Lighting estimation is often treated as a subcomponent of this broader task,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'start_index': 868}, page_content='from image observations. Lighting estimation is often treated as a subcomponent of this broader task,\\nwith prior work jointly estimating lighting alongside depth, normals, and albedo. Learning-based\\napproaches [48, 32, 58] typically leverage physics-based constraints and use re-rendering losses to\\nsupervise predictions. However, these methods often assume simplified reflectance models such as\\nLambertian shading, which limits their ability to handle complex lighting effects.\\nOptimization-based methods leverage differentiable rendering [ 4, 70, 69, 6, 59, 41, 18, 33] to\\njointly optimize lighting parameters and other scene attributes through photometric losses and\\nregularization terms. Some approaches [ 30] follow a decomposition-then-optimization strategy:\\nestimating geometry and albedo first, then solving for lighting via optimization. Other works\\nalso explore priors from proxy geometry [ 64] or pretrained general models [ 37, 35, 42]. The'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'start_index': 1633}, page_content='estimating geometry and albedo first, then solving for lighting via optimization. Other works\\nalso explore priors from proxy geometry [ 64] or pretrained general models [ 37, 35, 42]. The\\noptimization-based pipelines often require dense multi-view captures or known proxy geometry, and\\ninvolve expensive test-time optimization procedures. In contrast, our method directly predicts HDR\\nillumination in a feed-forward manner without requiring scene geometry or iterative inference.\\nDiffusion model priors.Diffusion models (DMs) have emerged as a powerful class of generative\\nmodels in high-fidelity image [45, 2, 46, 7] and video synthesis [21, 72, 3, 62, 1]. Beyond generation,\\npretrained DMs have been adapted to perception tasks through task-specific finetuning on carefully\\ncurated datasets [61, 38, 19], showing strong results on spatially aligned predictions such as depth [29,\\n24, 28], surface normals [13, 63, 34], albedo [11, 30, 67, 34], and material properties [30, 67, 34, 42].'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'start_index': 2515}, page_content='24, 28], surface normals [13, 63, 34], albedo [11, 30, 67, 34], and material properties [30, 67, 34, 42].\\nAdapting DMs to non-local tasks like lighting introduces new modeling challenges, as outputs such\\nas HDR panoramas are not spatially-aligned with the input.\\n3 Preliminaries: Diffusion Models\\nDiffusion models learn to approximate a data distribution pdata(x) through iterative denoising. Fol-\\nlowing DDPM [20], a forward process progressively adds Gaussian noise to a data samplex0 ∼ pdata,\\nproducing a noisy version at timestep t ∈ [1, T] as: xt = √¯αtx0 + √1 − ¯αtϵ, where ϵ ∼ N(0, I)\\nand ¯αt defines the noise schedule. During training, a neural network µθ learns to reverse this process\\nby minimizing:\\nEx0∼pdata(x),t∼pt,ϵ∼N(0,I)\\n\\x02\\n∥µθ(xt; c, t) − y∥2\\n2\\n\\x03\\n, (1)\\nwhere c represents optional conditioning inputs. The denoising target y varies by formulation, and\\ncan be the noise ϵ [20], the v-prediction √¯αtϵ − √1 − ¯αtx0 [47], or the clean signal x0 itself [27].'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'start_index': 3384}, page_content='can be the noise ϵ [20], the v-prediction √¯αtϵ − √1 − ¯αtx0 [47], or the clean signal x0 itself [27].\\nAt inference time, samples are generated by denoising an initial Gaussian sample through a fixed\\nnumber of reverse steps. In this paper, we build on CogVideoX [62], a latent video diffusion model\\ntrained on compressed video representations. A pretrained auto-encoder pair {E, D} maps RGB\\nvideos to and from a latent space, such that E(x) = z and D(z) ≈ x. All diffusion training and\\ngeneration is performed in this lower-dimensional latent space to reduce memory and computational.\\n4 Method\\nWe propose LuxDiT, a diffusion-based generative framework for estimating high-dynamic-range\\n(HDR) environment maps from a single image or video. We tailor a recent video diffusion transformer\\narchitecture [62] for lighting estimation, by jointly processing denoising targets (environment lighting)\\nand condition tokens (LDR input images) through self-attention layers. Since a single image can be'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'start_index': 4276}, page_content='and condition tokens (LDR input images) through self-attention layers. Since a single image can be\\ntreated as a one-frame video, we refer to both inputs uniformly as input video in the remainder of this\\nsection. An overview of the architecture is shown in Figure 2. In the following sections, we describe\\nthe model design, data sources, and training procedure.\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'start_index': 0}, page_content='Diffusion \\nTransformer\\nLoRA\\nNoneStage 1\\nStage 2\\nChannel \\nconcat.\\nInput Image/Video !\\nFrozen\\n\"!\"#\\n\"$%&\\n\"$!#\\nInput Env \\nPred. Env\\n#\"$!#\\n#\"$%&\\nHDR Env #\"\\nHDR \\nFusion \\nMLP\\nToken concat.\\nFigure 2: Method Overview.Given an input image or video I, LuxDiT predicts an environment map E as two\\ntone-mapped representations, Eldr and Elog, guided by a directional map Edir. Environment maps are encoded\\nwith a V AE, and the resulting latents are concatenated and jointly processed with visual input by a DiT. The\\noutputs Eldr and Elog are decoded and fused by a lightweight MLP to reconstruct the final HDR panorama.\\n4.1 Model Design\\nWe formulate HDR environment map estimation as a conditional denoising task. Given an input\\nvideo I ∈ RL×H×W×3 with L frames, the model generates a corresponding sequence of 360◦ HDR\\npanoramas E ∈ RL×He×We×3.\\nTwo core challenges arise: (1) standard V AEs used in latent diffusion models are trained on LDR'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'start_index': 806}, page_content='panoramas E ∈ RL×He×We×3.\\nTwo core challenges arise: (1) standard V AEs used in latent diffusion models are trained on LDR\\nimages and cannot faithfully encode HDR content, and (2) the output panoramas are not spatially\\naligned with the input, requiring flexible conditioning mechanisms. We address these challenges\\nusing a dual-tonemapping HDR representation, token-based conditioning, and a unified transformer\\narchitecture that jointly denoises two latent representations of lighting.\\nHDR lighting representation.Realistic lighting involves high-intensity components such as the sun\\nor artificial sources, with radiance values often exceeding 100 or 1,000. Representing this range in\\nlatent space is non-trivial: standard V AEs are trained on[0, 1]-normalized LDR images and cannot\\nreconstruct such dynamic content, and retraining on HDR data is impractical due to data scarcity\\nInspired by prior works [26, 34], we represent each HDR panorama E using two complementary\\ntonemapped representations:'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'start_index': 1687}, page_content='Inspired by prior works [26, 34], we represent each HDR panorama E using two complementary\\ntonemapped representations:\\nEldr = E\\n1 + E ·\\n\\x12\\n1 + E\\nM2\\nldr\\n\\x13\\n; Elog = log(1 + E)\\nlog(1 + Mlog) (2)\\nwhere Eldr is a standard Reinhard tonemapping and Elog captures normalized log-intensity. We set\\nMldr = 16 and Mlog = 10,000. Both outputs are clipped to [0, 1] before V AE encoding.\\nAt inference time, the HDR environment map is reconstructed using a lightweight MLP ψ:\\nˆE = ψ (Eldr, Elog) . (3)\\nDiffusion latents. Our model builds on a transformer-based diffusion model µθ, adapted to predict\\nHDR environment maps from visual input. The model operates in latent space and jointly denoises\\ntwo tonemapped representations of the HDR lighting.\\nThe tonemapped inputs Eldr and Elog are encoded by the pretrained V AE into latent tensors[zldr, zlog]\\nwith shape as Rl×he×we×C. These are concatenated along the channel dimension to form the diffusion'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'start_index': 2523}, page_content='with shape as Rl×he×we×C. These are concatenated along the channel dimension to form the diffusion\\ntarget z = [zldr, zlog] ∈ Rl×he×we×2C. The input and output projection layers of the diffusion network\\nµθ are extended to accommodate the increased channel dimension.\\nConditioning visual input in DiT.Accurate lighting estimation requires the model to extract fine-\\ngrained shading cues from the input image, such as shadow orientation, surface reflections, and\\nspecular highlights. Unlike pixel-aligned image-to-image translation tasks, we empirically observe\\nthat concatenating conditions to the noisy latents leads to poor performance (see Table 7), indicating\\nthe need for a more flexible conditioning mechanism.\\nTo this end, we adopt a fully attention-based architecture for the input video conditions. Specifically,\\nwe encode the input video I ∈ RL×H×W×3 into a latent tensor E(I) ∈ Rl×h×w×C using the pre-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'start_index': 3238}, page_content='To this end, we adopt a fully attention-based architecture for the input video conditions. Specifically,\\nwe encode the input video I ∈ RL×H×W×3 into a latent tensor E(I) ∈ Rl×h×w×C using the pre-\\ntrained V AE encoder, and flatten it into a token sequencec ∈ Rlhw×C. To help the model distinguish\\nbetween condition tokens and denoising targets, we apply separate adaptive layer normalization\\n(AdaLN) modules [43, 62] to each token type at every transformer block.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'start_index': 0}, page_content='Directional embedding. To improve angular continuity in the predicted panoramas, we inject\\ndirectional information into the model. Specifically, we construct a direction map of unit vectors\\nEdir that encodes per-pixel lighting directions in the camera coordinate system. This map is passed\\nthrough the same V AE encoderE, then projected and fused into the noise tokens using channel-wise\\nconcatenation before the transformer blocks. During training, we apply random horizontal rotations\\nto Edir to encourage rotational equi-variance and robust directional encoding.\\nConditioned denoising process.To put it together, at each denoising timestep t, the model receives\\na noisy latent zt = [zldr\\nt , zlog\\nt ] and predicts the corresponding clean latents conditioned on visual\\ninput as µθ(zt; c, t). This transformer-based design allows the model to propagate indirect lighting\\ncues—such as shadows and reflections—through global self-attention, enabling lighting prediction'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'start_index': 771}, page_content='input as µθ(zt; c, t). This transformer-based design allows the model to propagate indirect lighting\\ncues—such as shadows and reflections—through global self-attention, enabling lighting prediction\\nthat is both scene-consistent and directionally accurate.\\n4.2 Data Strategy\\nSupervised training of our model requires paired data in the form (I, Eldr, Elog), where I is an LDR\\ninput and Eldr, Elog are tonemapped versions of the target HDR environment map. To overcome the\\nscarcity of real-world HDR annotations, we leverage three complementary data sources: synthetic\\nrenderings, HDR panorama images, and LDR panoramic videos.\\nSynthetic rendering data.To supervise lighting prediction using physically accurate visual cues,\\nwe generate synthetic data by rendering randomized 3D scenes lit by HDR environment maps. Each\\nscene consists of (i) a ground plane with randomly assigned PBR materials, (ii) 3D objects sampled'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'start_index': 1494}, page_content='we generate synthetic data by rendering randomized 3D scenes lit by HDR environment maps. Each\\nscene consists of (i) a ground plane with randomly assigned PBR materials, (ii) 3D objects sampled\\nfrom Objaverse [10], and (iii) simple geometric primitives such as spheres, cubes, and cylinders\\nwith varied materials. We render multiple frames per scene with randomized camera trajectories and\\nenvironment map rotations. Despite their simplicity, these scenes exhibit diverse lighting effects,\\nincluding cast shadows, specular highlights, and inter-reflections, all paired with ground-truth HDR\\nillumination. Empirically, we find this data is critical for enabling the model to learn accurate shading\\ncues and light-source location (see Table 7).\\nHDR panorama images. We generate training pairs by sampling perspective crops from HDR\\nenvironment maps with data augmentation. Specifically, given a panorama, we randomly sample'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'start_index': 2237}, page_content='HDR panorama images. We generate training pairs by sampling perspective crops from HDR\\nenvironment maps with data augmentation. Specifically, given a panorama, we randomly sample\\ncamera parameters including azimuth, elevation, field of view, and exposure scale. These parameters\\ndefine a virtual pinhole camera, which we use to project the panorama into an LDR perspective view\\nI. The corresponding HDR environment map serves as the ground truth lighting target E. To support\\ntemporal training, we extend this procedure to generate multi-frame sequences by smoothly varying\\nthe camera pose over time.\\nLDR panorama videos.To enable the generation of dynamic panorama environment maps, we also\\nincorporate training data from LDR panoramic videos. Although ground-truth HDR environment\\nmaps are not available for this source, we use it in the form (I, Eldr, ∅), where Eldr is derived using\\ntonemapping and ∅ indicates the absence of log-space intensity. The panoramic video is projected'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'start_index': 3124}, page_content='tonemapping and ∅ indicates the absence of log-space intensity. The panoramic video is projected\\ninto a perspective-view video using randomized camera parameters, following the same procedure as\\nabove. Despite the lack of HDR intensity, this data improves robustness and temporal consistency\\nby exposing the model to natural image statistics, motion patterns, and diverse real-world lighting\\nconditions. We use 2,000 panoramic videos from the WEB360 dataset [56] for training, and hold out\\n114 videos for evaluation.\\n4.3 Training Scheme\\nWe adopt a two-stage training strategy to progressively build the model’s capacity and improve\\ngeneralization. The first stage focuses on learning physically grounded lighting cues from synthetic\\ndata. The second stage adapts the model to real-world distributions through LoRA-based fine-tuning.\\nStage I: Synthetic supervised training.We begin by training the model on the synthetic rendering'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'start_index': 3857}, page_content='data. The second stage adapts the model to real-world distributions through LoRA-based fine-tuning.\\nStage I: Synthetic supervised training.We begin by training the model on the synthetic rendering\\ndataset described in Section 4.2. This stage enables the model to learn the fundamental relationship\\nbetween image-based shading cues and HDR environment lighting.\\nWe follow the standard DDPM training objective [20] adopted by the CogVideoX base model [62]:\\nLI(θ) = Ez0,ϵ∼N(0,I),t∼U(T)\\n\\x02\\n∥ϵ − µθ(zt, c, t)∥2\\n2\\n\\x03\\n, (4)\\nwhere z0 denotes the clean latent pair [zldr, zlog], and c is the conditioning latent from the input video.\\nDuring training, we randomly drop eitherzldr or zlog with probability p = 0.1 to encourage robustness\\nto missing tonemapped representations.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'start_index': 0}, page_content='Table 1: Comparison of our method with baselines on three benchmark datasets. The results are\\nreported in terms of scale-invariant RMSE, angular error, and normalized RMSE.\\nDataset Method Scale-invariant RMSE↓ Angular Error↓ Normalized RMSE↓\\nDiffuse Matte Mirror Diffuse Matte Mirror Diffuse Matte Mirror\\nLaval Indoor\\nStyleLight 0.135 0.315 0.552 4.238 4.742 6.781 0.234 0.404 0.511\\nDiffusionLight 0.124 0.325 0.597 2.500 3.421 5.936 0.216 0.361 0.431\\nOurs 0.112 0.297 0.586 2.555 3.526 5.641 0.196 0.341 0.457\\nLaval Outdoor\\nH-G et al. [22] 0.300 0.437 0.587 7.851 8.755 26.052 0.551 0.627 0.740\\nNLFE 0.112 0.234 0.431 4.804 5.279 7.278 0.217 0.331 0.496\\nDiffusionLight 0.083 0.224 0.414 1.936 2.955 5.491 0.167 0.330 0.472\\nOurs 0.068 0.190 0.396 2.018 2.939 5.286 0.137 0.271 0.454\\nPoly Haven\\nStyleLight 0.138 0.336 0.620 3.034 4.272 6.602 0.198 0.344 0.474\\nNLFE 0.159 0.326 0.571 3.305 4.240 5.180 0.224 0.365 0.458\\nDiffusionLight 0.113 0.270 0.519 2.199 3.121 4.104 0.191 0.282 0.391'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'start_index': 794}, page_content='StyleLight 0.138 0.336 0.620 3.034 4.272 6.602 0.198 0.344 0.474\\nNLFE 0.159 0.326 0.571 3.305 4.240 5.180 0.224 0.365 0.458\\nDiffusionLight 0.113 0.270 0.519 2.199 3.121 4.104 0.191 0.282 0.391\\nOurs 0.077 0.196 0.442 1.235 1.977 2.783 0.111 0.199 0.323\\nTable 2: Angular error on estimated peak\\nluminance light direction on Laval Outdoor\\nsunny scenes.\\nMethod Peak Angular Error↓Mean Median\\nH-G et al. [22] 52.8 47.8NLFE 52.9 43.5DiffusionLight44.4 32.1Ours 23.7 17.5\\nTable 3: Quantitative comparison with video input. Peak an-\\ngular error (PAE) is used to evaluate PolyHaven-Peak videos.\\nAngular error (AE) on is used to evaluate WEB360 LDR videos.\\nMethod PolyHaven-Peak WEB360\\nPAE Mean↓ PAE Std↓ AE↓ AE Std↓\\nDiffusionLight 19.09 10.31 6.504 0.269\\nOurs (image) 5.74 3.68 5.679 0.382\\nOurs (video) 5.21 1.95 5.218 0.072\\nStage II: Semantic adaptation.After base training, we fine-tune the model to improve semantic\\nalignment between the input appearance and the predicted HDR environment map.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'start_index': 1610}, page_content='Stage II: Semantic adaptation.After base training, we fine-tune the model to improve semantic\\nalignment between the input appearance and the predicted HDR environment map.\\nThis stage uses real-world data sources, including perspective projections from HDR panoramas and\\nLDR panoramic videos. Since HDR ground truth is not available in the latter, we supervise only\\nthe LDR-tonemapped component. To avoid overfitting and preserve the pretrained model capacity,\\nwe apply parameter-efficient LoRA fine-tuning [ 23], optimizing a small set of injected low-rank\\nparameters ∆θ in the transformer layers:\\nLII(∆θ) = Ez0,ϵ∼N(0,I),t∼U(T)\\n\\x02\\n∥ϵ − µθ+∆θ(zt, c, t)∥2\\n2\\n\\x03\\n, (5)\\n5 Experiments\\n5.1 Experiment Settings\\nImplementation details. We use the pre-trained CogVideoX [ 62] model as our backbone. All\\ntraining is conducted on 16 NVIDIA A100 GPUs. Input resolutions are randomly sampled be-\\ntween 512×512 and 480×720, and output environment map resolutions are between 128×256 and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'start_index': 2401}, page_content='training is conducted on 16 NVIDIA A100 GPUs. Input resolutions are randomly sampled be-\\ntween 512×512 and 480×720, and output environment map resolutions are between 128×256 and\\n256×512. The image-based model is trained with a batch size of 192 for 12,000 iterations. For video\\ntraining, we use the same spatial resolutions and uniformly sample frame lengths from 9, 17, 25. The\\nvideo model is trained with an average batch size of 48 for an additional 12,000 iterations. LoRA\\nmodules are applied to all attention layers with a rank of 64. We fine-tune the LoRA parameters for\\n5,000 iterations during the adaptation stage. Please refer to supplement for implementation details.\\nDatasets. We evaluate our method on the following three benchmark datasets, covering various\\nindoor and outdoor scenes. 1) Laval Indoor [15]: We use the same set of 289 test HDRIs used by prior\\nworks [44, 54]; 2) Laval Outdoor [22]: We evaluate on 116 sunny HDR panoramas with concentrated'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'start_index': 3173}, page_content='indoor and outdoor scenes. 1) Laval Indoor [15]: We use the same set of 289 test HDRIs used by prior\\nworks [44, 54]; 2) Laval Outdoor [22]: We evaluate on 116 sunny HDR panoramas with concentrated\\nsunlight selected from the original dataset; 3) Poly Haven [66]: We select 181 Poly Haven HDRIs\\nnot used during model training to evaluate performance across both indoor and outdoor scenes.\\nMetrics. Following prior works [54, 44], we use three standard metrics for evaluating HDR lighting:\\nscale-invariant root mean square error (si-RMSE) [17], angular error in degrees [31], and normalized\\nRMSE (n-RMSE) [44]. For scenes with concentrated sunlight, we additionally report peak angular\\nerror (PAE) [22, 57], which measures the angular deviation of the predicted peak light direction.\\nBaselines. For indoor scenes, we compare against DiffusionLight [44], StyleLight [54], Weber et\\nal. [60], and EMLight [68], using metrics reported by [44] when applicable. For outdoor scenes, we'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'start_index': 3954}, page_content='Baselines. For indoor scenes, we compare against DiffusionLight [44], StyleLight [54], Weber et\\nal. [60], and EMLight [68], using metrics reported by [44] when applicable. For outdoor scenes, we\\ncompare against DiffusionLight [44], Hold-Geoffroy et al. [22], and NLFE [57].\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'start_index': 0}, page_content='Input Image GT StyleLight DiffusionLight Ours Input Image GT StyleLight DiffusionLight Ours\\nInput Image GT H-G et al. DiffusionLight Ours Input Image GT H-G et al. DiffusionLight Ours\\nInput Image GT NLFE DiffusionLight Ours Input Image GT NLFE DiffusionLight Ours\\nFigure 3: Qualitative comparison with baseline methods on three benchmark datasets.\\nReference StyleLight DiffusionLight DiPIR Ours\\nFigure 4: Qualitative comparison of virtual object insertion.\\nTable 4: Ablation study on impact\\nof LoRA scale at inference time.\\nLoRA Scale Diffuse↓ Matte↓ Mirror↓\\n0.00 2.98 5.02 6.07\\n0.25 2.09 3.69 4.67\\n0.50 1.52 2.66 3.56\\n0.75 1.22 2.05 2.88\\n1.00 1.17 1.92 2.72\\nTable 5: Ablation study on im-\\npact of camera field-of-view.\\nFOV Diffuse↓ Matte↓ Mirror↓\\n45◦ 1.29 2.14 2.95\\n50◦ 1.26 2.06 2.86\\n60◦ 1.17 1.92 2.72\\n70◦ 1.15 1.85 2.63\\n75◦ 1.13 1.80 2.59\\nTable 6: Ablation study on impact\\nof camera elevation.\\nElevation Diffuse↓ Matte↓ Mirror↓\\n−30◦ 1.70 3.04 3.95\\n−15◦ 1.22 2.05 2.87\\n+00◦ 1.17 1.92 2.72'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'start_index': 805}, page_content='70◦ 1.15 1.85 2.63\\n75◦ 1.13 1.80 2.59\\nTable 6: Ablation study on impact\\nof camera elevation.\\nElevation Diffuse↓ Matte↓ Mirror↓\\n−30◦ 1.70 3.04 3.95\\n−15◦ 1.22 2.05 2.87\\n+00◦ 1.17 1.92 2.72\\n+15◦ 1.28 2.09 2.94\\n+30◦ 1.71 2.59 3.51\\nTable 7: Ablation study on model design choices and training data. We report the angular error with three-\\nspheres protocol.\\nSettings Laval Indoor Poly Haven\\nDiffuse↓ Matte↓ Mirror↓ Diffuse↓ Matte↓ Mirror↓\\nOurs (channel concat.) 7.09 10.04 11.07 7.09 10.04 11.07\\nOurs (w/o synthetic data)4.50 5.14 6.96 1.48 2.08 2.86\\nOurs 2.56 3.53 5.64 1.23 1.98 2.78\\n5.2 Evaluation of Image Lighting Estimation\\nWe follow the evaluation protocol from prior work to render spheres with three representative\\nmaterials (gray-diffuse, silver-matte, and mirror), using the estimated HDR environment map from\\nthe LDR input image [15, 54, 44].\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'start_index': 0}, page_content='Frame 0 Frame 12 Frame 24 Frame 0 Frame 12 Frame 24\\n0 12 24 0 12 24 0 12 24 0 12 24\\nReference DiffusionLight Reference DiffusionLight\\nOurs image Ours video Ours image Ours video\\nFigure 5: Qualitative comparison of video lighting estimation.\\nTable 1 reports quantitative comparisons on three benchmarks spanning both indoor and outdoor\\nscenes. On the Laval Indoor dataset, our method performs comparably or better than DiffusionLight\\nacross most metrics, despite not using Laval Indoor dataset during training. This dataset exhibits\\na noticeable shift in color and intensity distribution compared to our training set, and our strong\\nperformance demonstrates robust generalization.\\nFrom qualitative comparison shown in Figure 3, DiffusionLight can lose angular high-frequency\\ndetails from the input image due to its distorted representation. In contrast, our estimated environment\\nmaps can recover more high-frequency details while preserving accurate lighting.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'start_index': 774}, page_content='details from the input image due to its distorted representation. In contrast, our estimated environment\\nmaps can recover more high-frequency details while preserving accurate lighting.\\nOn the Laval Outdoor and Poly Haven datasets with a broader dynamic range, our method consistently\\noutperforms prior state-of-the-art methods. Hold-Geoffroy et al. [22] can estimate concentrated\\npeak light source such as sunlight; however, its results do not adapt well to the details of the input\\nimage. NLFE [57] can estimate in-context environment maps, but it often fails to estimate accurate\\nhighlights. DiffusionLight performs better than other baselines, but due to its limited dynamic range,\\nit struggles with outdoor high-intensity light sources.\\nTo further assess directional accuracy, we evaluate the angular error of the peak luminance direction\\non a subset of the Laval Outdoor dataset containing direct sunlight. Table 2 reports the mean and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'start_index': 1516}, page_content='To further assess directional accuracy, we evaluate the angular error of the peak luminance direction\\non a subset of the Laval Outdoor dataset containing direct sunlight. Table 2 reports the mean and\\nmedian peak angular errors. Our method reduces peak angular error by nearly 50% compared to\\nDiffusionLight, confirming its advantage in capturing accurate light direction—a critical factor for\\ncasting realistic shadows in downstream applications such as object insertion.\\n5.3 Evaluation of Video Lighting Estimation\\nTo evaluate lighting estimation accuracy and consistency on video input, we construct two types of\\ntest sequences:\\n• PolyHaven-Peak: We project 12 unseen Poly Haven panoramas (each with direct sunlight) into\\nvideos using a smooth panning camera. This setting is used to evaluate peak angular error.\\n• WEB360: We randomly select 12 LDR panoramic videos featuring dynamic content from\\nWEB360 and render them into perspective views with fixed horizontal camera motion. This'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'start_index': 2331}, page_content='• WEB360: We randomly select 12 LDR panoramic videos featuring dynamic content from\\nWEB360 and render them into perspective views with fixed horizontal camera motion. This\\nsetting evaluates temporal consistency using chromatic angular error on rendered mirror spheres.\\nEach set contains 12 videos at resolution of480×720 and a length of 25 frames. To quantify temporal\\nconsistency, we compute the standard deviation (std) of per-frame error metrics for each video clip,\\nand average the results across the 12-video set.\\nWe compare our video inference to two baselines: our own image-based inference (applied frame-by-\\nframe) and DiffusionLight [44]. Table 3 reports the results. Our method outperforms DiffusionLight.\\nComparing to Ours (image), video inference achieves higher accuracy and significantly lower\\ntemporal variance, indicating more stable predictions across time.\\nFigure 5 shows qualitative examples of video inference. Both DiffusionLight and our image-based'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'start_index': 3140}, page_content='temporal variance, indicating more stable predictions across time.\\nFigure 5 shows qualitative examples of video inference. Both DiffusionLight and our image-based\\nvariant exhibit visible temporal flickering. In contrast, our method produces smooth lighting transi-\\ntions, successfully aligning content across frames and preserving consistent lighting behavior over\\ntime.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'start_index': 0}, page_content='5.4 Evaluation of Virtual Object Insertion\\nTable 8: Quantitative evaluation of virtual\\nobject insertion. We report the percentage of\\nimages where users preferred Ours over base-\\nlines. A preference > 50% indicates Ours\\noutperforming baselines.\\nMethod RMSE ↓SSIM↑Ours Preferred\\nStyleLight 0.056 0.986 60.6%\\nDiffusionLight 0.057 0.987 60.6%\\nDiPIR 0.048 0.989 54.5%\\nOurs 0.047 0.990 /\\nVirtual object insertion is a key downstream applica-\\ntion of lighting estimation. We evaluate our method on\\nthis task using the benchmark from [35], using 11 HDR\\npanoramas from the Poly Haven dataset [66]. For each\\nscene, a virtual object and a known ground plane are\\nmanually placed into the environment. Each test case\\nincludes an LDR background image rendered from the\\nHDR panorama, along with a posed object and ground\\nplane. A pseudo-ground-truth object insertion is gen-\\nerated by rendering the object using the original HDR\\nenvironment map. This allows for controlled comparison'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'start_index': 806}, page_content='plane. A pseudo-ground-truth object insertion is gen-\\nerated by rendering the object using the original HDR\\nenvironment map. This allows for controlled comparison\\nagainst renderings produced using predicted lighting.\\nWe report quantitative metrics in Table 8. In addition, we conduct a user study to assess perceptual\\nquality (details provided in the supplement), and report the percentage of samples where users\\npreferred our results over baseline methods.\\nOur method achieves visual quality comparable to DiPIR and significantly outperforms other baselines.\\nNotably, DiPIR is specialized for object insertion and incorporates additional modules for tone\\nmapping and appearance harmonization. In contrast, our model estimates lighting alone, yet still\\nproduces realistic composite renderings. We include qualitative results in Figure 4.\\n5.5 Ablation Study\\nModel Design and Training DataWe evaluate two model variants to ablate the contributions'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'start_index': 1559}, page_content='produces realistic composite renderings. We include qualitative results in Figure 4.\\n5.5 Ablation Study\\nModel Design and Training DataWe evaluate two model variants to ablate the contributions\\nof our architectural and training design: (1) Channel concatenation: This variant fuses input and\\nenvironment map (resized to match input image) latents along the channel dimension [ 26], and\\nno token-wise concatenation is used. Our two-stage training is also applied. (2) Training without\\nsynthetic data: This variant skips Stage I training and uses only panorama crops for fine-tuning.\\nTable 7 reports angular errors on Laval Indoor and Poly Haven. Channel concatenation significantly\\nunderperforms, confirming the importance of token-level conditioning. Without synthetic pretraining,\\nthe model performs well in-domain (Poly Haven) but degrades out-of-domain (Laval Indoor), showing\\nsynthetic data pre-training is crucial for learning generalized lighting priors.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'start_index': 2340}, page_content='the model performs well in-domain (Poly Haven) but degrades out-of-domain (Laval Indoor), showing\\nsynthetic data pre-training is crucial for learning generalized lighting priors.\\nLoRA scale. We vary the LoRA interpolation weight from 0.0 to 1.0 to ablate how fine-tuned LoRA\\naffects the predicted lighting content. Table 4 shows that higher LoRA weights yield lower angular\\nerror on Poly Haven, validating the effectiveness of LoRA for improving semantic alignment.\\nCamera sensitivity. We test robustness to camera variation by rendering crops from Poly Haven\\nunder varying field of view (45◦ to 75◦) and camera elevation (−30◦ to 30◦). Results in Tables 5 and\\n6 show that while extreme viewpoints introduce mild error increases, performance remains stable,\\ndemonstrating robustness to moderate viewpoint shifts.\\n6 Discussion\\nWe introduce LuxDiT, a conditional generative model for estimating HDR scene illumination from'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'start_index': 3098}, page_content='demonstrating robustness to moderate viewpoint shifts.\\n6 Discussion\\nWe introduce LuxDiT, a conditional generative model for estimating HDR scene illumination from\\ncasually captured images and videos. Our approach fine-tunes a video diffusion transformer (DiT)\\nto synthesize HDR environment maps, combining large-scale synthetic data for learning physically\\ngrounded priors with LoRA-based adaptation on real HDR panoramas to improve semantic alignment.\\nExtensive experiments demonstrate that LuxDiT produces accurate, high-frequency, and scene-\\nconsistent lighting predictions from limited visual input.\\nLimitations and future work.While LuxDiT produces high-quality lighting predictions, inference\\nremains computationally intensive due to the iterative nature of diffusion models, limiting its use in\\nreal-time applications. Future work could explore model distillation or more efficient architectures'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'start_index': 3900}, page_content='real-time applications. Future work could explore model distillation or more efficient architectures\\nto accelerate inference. Additionally, the resolution of predicted panoramas is limited by data and\\ntraining scale; generating high-resolution outputs for immersive applications will require richer, more\\ndiverse HDR supervision. Looking ahead, with recent progress in joint generative modeling [5, 36],\\nwe see LuxDiT as a step toward unified inverse and forward rendering frameworks, complementing\\nrecent progress in neural forward rendering and G-buffer estimation [ 67, 34]. Future directions\\ninclude joint modeling or co-training of lighting, geometry, and material for general-purpose scene\\nreconstruction and appearance synthesis.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'start_index': 0}, page_content='References\\n[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay,\\nYongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv\\npreprint arXiv:2501.03575, 2025.\\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis,\\nMiika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. eDiff-I:\\ntext-to-image diffusion models with ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\\n[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and\\nKarsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n[4] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik P.A. Lensch. NeRD:'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'start_index': 765}, page_content='Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n[4] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik P.A. Lensch. NeRD:\\nneural reflectance decomposition from image collections. In ICCV, 2021.\\n[5] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and\\nShelly Sheynin. VideoJAM: Joint appearance-motion representations for enhanced motion generation in\\nvideo models. arXiv: 2502.02492, 2025.\\n[6] Wenzheng Chen, Joey Litalien, Jun Gao, Zian Wang, Clement Fuji Tsang, Sameh Khalis, Or Litany, and\\nSanja Fidler. DIB-R++: Learning to predict lighting and material with a hybrid differentiable renderer. In\\nNeurIPS, 2021.\\n[7] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon\\nVandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using\\nphotogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'start_index': 1571}, page_content='Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using\\nphotogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023.\\n[8] Mohammad Reza Karimi Dastjerdi, Jonathan Eisenmann, Yannick Hold-Geoffroy, and Jean-François\\nLalonde. Everlight: Indoor-outdoor editable hdr lighting estimation. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV), pages 7420–7429, October 2023.\\n[9] Paul E. Debevec and Jitendra Malik. Recovering high dynamic range radiance maps from photographs.\\nIn Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIG-\\nGRAPH ’97, page 369–378, USA, 1997. ACM Press/Addison-Wesley Publishing Co.\\n[10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt,\\nKiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3D objects.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'start_index': 2406}, page_content='Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3D objects.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13142–\\n13153, 2023.\\n[11] Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and Anand Bhattad. Generative models: What do\\nthey know? do they know things? let’s find out!, 2024.\\n[12] Egor Ershov, Alexey Savchik, Illya Semenkov, Nikola Bani´c, Alexander Belokopytov, Daria Senshina,\\nKarlo Košˇcevi´c, Marko Subaši´c, and Sven Lonˇcari´c. The cube++ illumination estimation dataset. IEEE\\naccess, 8:227511–227527, 2020.\\n[13] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao\\nLong. GeoWizard: unleashing the diffusion priors for 3D geometry estimation from a single image. In\\nECCV, 2024.\\n[14] Marc-André Gardner, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Christian Gagné, and Jean-François'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'start_index': 3213}, page_content='ECCV, 2024.\\n[14] Marc-André Gardner, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Christian Gagné, and Jean-François\\nLalonde. Deep parametric indoor lighting estimation. In ICCV, pages 7175–7183, 2019.\\n[15] Marc-André Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen, Emiliano Gambaretto, Christian\\nGagné, and Jean-François Lalonde. Learning to predict indoor illumination from a single image. arXiv\\npreprint arXiv:1704.00090, 2017.\\n[16] Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, and Jean-François Lalonde. Fast spatially-\\nvarying indoor lighting estimation. In CVPR, pages 6908–6917, 2019.\\n[17] Roger Grosse, Micah K. Johnson, Edward H. Adelson, and William T. Freeman. Ground truth dataset and\\nbaseline evaluations for intrinsic image algorithms. In ICCV, pages 2335–2342. IEEE, 2009.\\n[18] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. Shape, light, and material decomposition from\\nimages using Monte Carlo rendering and denoising. arXiv:2206.03380, 2022.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'start_index': 0}, page_content='[19] Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu,\\nand Ying-Cong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction.\\narXiv preprint arXiv:2409.18124, 2024.\\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\\nInformation Processing Systems, 33:6840–6851, 2020.\\n[21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.\\nVideo diffusion models. arXiv:2204.03458, 2022.\\n[22] Yannick Hold-Geoffroy, Akshaya Athawale, and Jean-François Lalonde. Deep sky modeling for single\\nimage outdoor lighting estimation. In CVPR, pages 6927–6935, 2019.\\n[23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on\\nLearning Representations, 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'start_index': 825}, page_content='Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on\\nLearning Representations, 2022.\\n[24] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan.\\nDepthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR, 2025.\\n[25] Wenzel Jakob, Sébastien Speierer, Nicolas Roussel, Merlin Nimier-David, Delio Vicini, Tizian Zeltner,\\nBaptiste Nicolet, Miguel Crespo, Vincent Leroy, and Ziyi Zhang. Mitsuba 3 renderer, 2022. https://mitsuba-\\nrenderer.org.\\n[26] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah\\nSnavely. Neural gaffer: Relighting any object via diffusion. In Advances in Neural Information Processing\\nSystems, 2024.\\n[27] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\\ngenerative models. In Proc. NeurIPS, 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'start_index': 1588}, page_content='Systems, 2024.\\n[27] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based\\ngenerative models. In Proc. NeurIPS, 2022.\\n[28] Bingxin Ke, Dominik Narnhofer, Shengyu Huang, Lei Ke, Torben Peters, Katerina Fragkiadaki, Anton\\nObukhov, and Konrad Schindler. Video depth without video models. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition (CVPR), 2025.\\n[29] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler.\\nRepurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\\n[30] Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for single-view material\\nestimation. In arxiv, 2023.\\n[31] Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel, Jay Busch, and Paul'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'start_index': 2412}, page_content='estimation. In arxiv, 2023.\\n[31] Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel, Jay Busch, and Paul\\nDebevec. Deeplight: Learning illumination for unconstrained mobile mixed reality. In CVPR, pages\\n5918–5928, 2019.\\n[32] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker.\\nInverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from a single\\nimage. In CVPR, pages 2475–2484, 2020.\\n[33] Ruofan Liang, Huiting Chen, Chunlin Li, Fan Chen, Selvakumar Panneer, and Nandita Vijaykumar.\\nEnvidr: Implicit differentiable renderer with neural environment lighting. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 79–89, 2023.\\n[34] Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander\\nKeller, Nandita Vijaykumar, Sanja Fidler, and Zian Wang. Diffusionrenderer: Neural inverse and forward'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'start_index': 3270}, page_content='Keller, Nandita Vijaykumar, Sanja Fidler, and Zian Wang. Diffusionrenderer: Neural inverse and forward\\nrendering with video diffusion models. arXiv preprint arXiv: 2501.18590, 2025.\\n[35] Ruofan Liang, Zan Gojcic, Merlin Nimier-David, David Acuna, Nandita Vijaykumar, Sanja Fidler, and\\nZian Wang. Photorealistic object insertion with diffusion-guided inverse rendering. In ECCV, 2024.\\n[36] Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao\\nYao, and Shiwei Li. Matrix3D: Large Photogrammetry Model All-in-One, 2025.\\n[37] Linjie Lyu, Ayush Tewari, Marc Habermann, Shunsuke Saito, Michael Zollhöfer, Thomas Leimkühler,\\nand Christian Theobalt. Diffusion posterior illumination for ambiguity-aware inverse rendering. ACM\\nTransactions on Graphics, 42(6), 2023.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'start_index': 0}, page_content='[38] Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, and\\nBastian Leibe. Fine-tuning image-conditional diffusion models is easier than you think. In Proceedings of\\nthe IEEE/CVF Winter Conference on Applications of Computer Vision (W ACV), 2025.\\n[39] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta.\\nObject 3dit: Language-guided 3d-aware image editing. Advances in Neural Information Processing\\nSystems, 36:3497–3516, 2023.\\n[40] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.\\nNeRF: representing scenes as neural radiance fields for view synthesis. arXiv preprint arXiv:2003.08934,\\n2020.\\n[41] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Müller,\\nand Sanja Fidler. Extracting triangular 3D models, materials, and lighting from images. arXiv:2111.12503,\\n2021.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'start_index': 840}, page_content='and Sanja Fidler. Extracting triangular 3D models, materials, and lighting from images. arXiv:2111.12503,\\n2021.\\n[42] Jacob Munkberg, Zian Wang, Ruofan Liang, Tianchang Shen, and Jon Hasselgren. VideoMat: Extracting\\nPBR Materials from Video Diffusion Models. In Eurographics Symposium on Rendering - CGF Track,\\n2025.\\n[43] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the\\nIEEE/CVF international conference on computer vision, pages 4195–4205, 2023.\\n[44] Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Amit Raj, Varun Jampani,\\nPramook Khungurn, and Supasorn Suwajanakorn. DiffusionLight: light probes for free by painting a\\nchrome ball. In ArXiv, 2023.\\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'start_index': 1668}, page_content='image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), 2022.\\n[46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\\nimage diffusion models with deep language understanding. Advances in Neural Information Processing\\nSystems, 35:36479–36494, 2022.\\n[47] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In The\\nTenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,\\n2022. OpenReview.net, 2022.\\n[48] Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, and Jan Kautz. Neural\\ninverse rendering of an indoor scene from a single image. In ICCV, 2019.\\n[49] Shuran Song and Thomas Funkhouser. Neural illumination: Lighting prediction for indoor environments.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'start_index': 2471}, page_content='inverse rendering of an indoor scene from a single image. In ICCV, 2019.\\n[49] Shuran Song and Thomas Funkhouser. Neural illumination: Lighting prediction for indoor environments.\\nIn CVPR, pages 6918–6926, 2019.\\n[50] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Alek-\\nsei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust\\nlarge mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on\\napplications of computer vision, pages 2149–2159, 2022.\\n[51] Jiajun Tang, Yongjie Zhu, Haoyu Wang, Jun-Hoong Chan, Si Li, and Boxin Shi. Estimating spatially-\\nvarying lighting in urban scenes with disentangled representation. In ECCV, 2022.\\n[52] Benjamin Ummenhofer, Sanskar Agrawal, Rene Sepulveda, Yixing Lao, Kai Zhang, Tianhang Cheng,\\nStephan Richter, Shenlong Wang, and German Ros. Objects with lighting: A real-world dataset for'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'start_index': 3223}, page_content='[52] Benjamin Ummenhofer, Sanskar Agrawal, Rene Sepulveda, Yixing Lao, Kai Zhang, Tianhang Cheng,\\nStephan Richter, Shenlong Wang, and German Ros. Objects with lighting: A real-world dataset for\\nevaluating reconstruction and rendering for object relighting. In 2024 International Conference on 3D\\nVision (3DV), pages 137–147. IEEE, 2024.\\n[53] Giuseppe Vecchio and Valentin Deschaintre. Matsynth: A modern pbr materials dataset. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\\n[54] Guangcong Wang, Yinuo Yang, Chen Change Loy, and Ziwei Liu. Stylelight: Hdr panorama generation\\nfor lighting estimation and editing. In European Conference on Computer Vision (ECCV), 2022.\\n[55] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning\\nneural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS, 2021.\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'start_index': 0}, page_content='[56] Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, and Jian Zhang. 360dvd: Controllable panorama\\nvideo generation with 360-degree video diffusion model. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, pages 6913–6923, 2024.\\n[57] Zian Wang, Wenzheng Chen, David Acuna, Jan Kautz, and Sanja Fidler. Neural light field estimation for\\nstreet scenes with differentiable virtual object insertion. In ECCV, 2022.\\n[58] Zian Wang, Jonah Philion, Sanja Fidler, and Jan Kautz. Learning indoor inverse rendering with 3D\\nspatially-varying lighting. In ICCV, 2021.\\n[59] Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob Munkberg, Jon Hasselgren, Zan Gojcic,\\nWenzheng Chen, and Sanja Fidler. Neural fields meet explicit geometric representations for inverse\\nrendering of urban scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\\nJune 2023.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'start_index': 788}, page_content='rendering of urban scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\\nJune 2023.\\n[60] Henrique Weber, Mathieu Garon, and Jean-François Lalonde. Editable indoor lighting estimation. In\\nEuropean Conference on Computer Vision, pages 677–692. Springer, 2022.\\n[61] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and\\nChunhua Shen. What matters when repurposing diffusion models for general dense perception tasks?\\narXiv preprint arXiv:2403.06090, 2024.\\n[62] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi\\nHong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert\\ntransformer. arXiv preprint arXiv:2408.06072, 2024.\\n[63] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang\\nXiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'start_index': 1561}, page_content='[63] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang\\nXiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM\\nTransactions on Graphics, 2024.\\n[64] Hong-Xing Yu, Samir Agarwala, Charles Herrmann, Richard Szeliski, Noah Snavely, Jiajun Wu, and\\nDeqing Sun. Accidental light probes. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pages 12521–12530, 2023.\\n[65] Ye Yu and William A. P. Smith. InverseRenderNet: learning single image inverse rendering. In CVPR,\\n2019.\\n[66] Greg Zaal and et al. Poly Haven - The Public 3D Asset Library, 2025.\\n[67] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-\\nQi Yan, and Miloš Hašan. RGB↔X: image decomposition and synthesis using material-and lighting-aware\\ndiffusion models. In ACM SIGGRAPH 2024 Conference Papers, pages 1–11, 2024.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'start_index': 2330}, page_content='Qi Yan, and Miloš Hašan. RGB↔X: image decomposition and synthesis using material-and lighting-aware\\ndiffusion models. In ACM SIGGRAPH 2024 Conference Papers, pages 1–11, 2024.\\n[68] Fangneng Zhan, Changgong Zhang, Yingchen Yu, Yuan Chang, Shijian Lu, Feiying Ma, and Xuansong\\nXie. Emlight: Lighting estimation via spherical distribution approximation. In Proceedings of the AAAI\\nConference on Artificial Intelligence, 2021.\\n[69] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. PhySG: Inverse rendering with\\nspherical Gaussians for physics-based material editing and relighting. In CVPR, 2021.\\n[70] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect\\nillumination for inverse rendering. In CVPR, 2022.\\n[71] Yiqin Zhao and Tian Guo. Pointar: Efficient lighting estimation for mobile augmented reality. arXiv\\npreprint arXiv:2004.00006, 2020.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'start_index': 3046}, page_content='illumination for inverse rendering. In CVPR, 2022.\\n[71] Yiqin Zhao and Tian Guo. Pointar: Efficient lighting estimation for mobile augmented reality. arXiv\\npreprint arXiv:2004.00006, 2020.\\n[72] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient\\nvideo generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.\\n[73] Yongjie Zhu, Yinda Zhang, Si Li, and Boxin Shi. Spatially-varying outdoor lighting estimation from\\nintrinsics. In CVPR, 2021.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14', 'start_index': 0}, page_content='Supplement for LuxDiT: Lighting Estimation with\\nVideo Diffusion Transformer\\nIn the supplementary material, we discuss the broader impact of our project in Sec. A, and provide\\nadditional details for implementation and experiments in Sec. B. Sec. C provides additional quantita-\\ntive and qualitative results. We refer to the accompanied video for extended comparisons on video\\nlighting estimation.\\nA Broader Impact\\nWe introduce LuxDiT, a generative model for estimating high-dynamic-range (HDR) environment\\nlighting from casually captured images and videos. Lighting estimation is a core challenge in\\nphotorealistic rendering due to its non-local and indirect nature. LuxDiT produces scene-consistent\\nHDR panoramas, enabling applications in virtual object insertion, relighting, AR/VR, and visual\\neffects. It can also support synthetic data generation for downstream tasks in robotics and perception,\\nwhere realistic illumination is critical.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14', 'start_index': 795}, page_content='effects. It can also support synthetic data generation for downstream tasks in robotics and perception,\\nwhere realistic illumination is critical.\\nSimilar to other generative methods, LuxDiT could be misused to produce visually convincing\\nbut deceptive content. While it does not directly generate synthetic scenes, it enables realistic\\nvirtual object insertion and may facilitate the creation of manipulated imagery that is difficult to\\ndistinguish from real footage. We encourage responsible use of LuxDiT and caution against its\\ndeployment in contexts where synthetic content could mislead viewers or undermine public trust,\\nsuch as misinformation or falsified media.\\nB Additional Details\\nB.1 HDR Reconstruction\\nSection 4.1 describes our method for reconstructing HDR environment maps from two tone-mapped\\nLDR images using a lightweight MLP ψ (Eldr, Elog). This MLP consists of 5 layers with 64 hidden'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14', 'start_index': 1509}, page_content='Section 4.1 describes our method for reconstructing HDR environment maps from two tone-mapped\\nLDR images using a lightweight MLP ψ (Eldr, Elog). This MLP consists of 5 layers with 64 hidden\\nunits per layer and LeakyReLU activation. A softplus activation is applied to the final output layer to\\nensure non-negative outputs.\\nThe MLP ψ operates on a per-pixel basis: it takes a pair of LDR RGB values as input and predicts a\\nsingle HDR RGB value. It is trained using the same HDR environment maps as the diffusion model,\\nwith augmentations including random intensity rescaling and exposure adjustments for diversity. To\\nsimulate limited input precision, LDR inputs are randomly quantized to 8-bit RGB values. We train\\nthe MLP using a Huber loss with δ = 1.0, which provides robustness against large HDR outliers\\nwhile preserving smooth gradients.\\nAdditionally, we show the tone-mapping curves used to generate the LDR images in Fig. 6. Our'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14', 'start_index': 2318}, page_content='while preserving smooth gradients.\\nAdditionally, we show the tone-mapping curves used to generate the LDR images in Fig. 6. Our\\ndual-tone mapping strategy ensures sufficient sampling across the full dynamic range [0, 10,000],\\nsupporting accurate HDR reconstruction.\\n0 1 4 8 16 100 1000 10000\\nHDR Value\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0LDR Value\\nReinhard\\nLogarithm\\nFigure 6: The two tone-mapping curves used to gen-\\nerate the LDR images. The 128 dot points along the\\ncurve are evenly spaced along [0, 1] LDR value range.\\n0.04 2.93 (50%) 837.67 (max)\\n99.9-th Percentile Intensity (Log Scale)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160Frequency\\nFigure 7: The histogram of the 99.9-th percentile\\nintensity of all HDR environment maps in our training\\nset.\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'start_index': 0}, page_content='B.2 Datasets\\nWe provide more details about the datasets used in our experiments.\\nThe data sources of HDR environment maps.We collected 2386 HDR environment maps from\\nthe following 4 data sources either publicly available or commercially available.\\n• Poly Haven0: 626 HDR environment maps with a wide range of indoor and outdoor lighting.\\n• HDR Maps1: 403 HDR environment maps with diverse lighting conditions, including 294\\npanorama maps and 109 hemi-sphere sky maps.\\n• HDRI Skies2: 457 HDR environment maps with outdoor lighting conditions.\\n• DOSCH DESIGN3: 900 HDR environment maps mainly for outdoor lighting conditions.\\nFigure 7 shows the histogram of the 99.9-th percentile intensity of all HDR environment maps in\\nour training set. With over 50% of the HDR environment maps having a 99.9-th percentile intensity\\ngreater than 2.93. Note that the for outdoor lighting, the highest intensity can be orders of magnitude'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'start_index': 818}, page_content='greater than 2.93. Note that the for outdoor lighting, the highest intensity can be orders of magnitude\\nhigher than the 99.9-th percentile. Among these, Poly Haven and HDR Maps offer greater diversity\\nacross scene types. To balance the training distribution across data sources, we apply sampling\\nweights in the ratio 3:2:2:1 in the order listed above.\\nFor quantitative and qualitative evaluation, we use the Laval Indoor4 and Laval Outdoor 5 datasets,\\nwhich contain calibrated HDR panoramas of real-world indoor and outdoor scenes.\\nFigure 8: Randomly sampled example images\\nfrom our synthetic rendering data.\\nSynthetic rendering data.Similar to OBJect [39]\\nand DiffusionRenderer [34], we create synthetic 3D\\nscenes by compositing multiple 3D objects from Ob-\\njaverse [10] and randomly placing them on a plane\\nwith varying plane textures. We use a filtered sub-\\nset of Objaverse, containing ∼269,000 3D objects\\nwith decent geometries and material textures, to cre-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'start_index': 1628}, page_content='with varying plane textures. We use a filtered sub-\\nset of Objaverse, containing ∼269,000 3D objects\\nwith decent geometries and material textures, to cre-\\nate synthetic 3D scenes. The varying plane textures\\nare sampled from ∼4000 PBR textures from Mat-\\nSynth6 [53]. Each composited scene contains up to\\n3 sampled Objaverse objects. We additionally add up\\nto 3 random geometry primitives (sphere, cube, and\\ncylinder) with varying material textures to provide\\nrich shading cues for model to learn. For each scene,\\nwe randomly render 1 ∼4 video clips with varying\\ncamera motions (e.g., orbiting camera and oscilating\\ncamera) and environment lightings. We use a path-tracing renderer with 128 samples per pixel (spp)\\nand the default OptiX denoiser to render the video clips with a resolution of 480 × 720 or 512 × 512.\\nThe HDR rendering results are tone-mapped to LDR images using Blender’s AgX tonemapping7. In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'start_index': 2341}, page_content='and the default OptiX denoiser to render the video clips with a resolution of 480 × 720 or 512 × 512.\\nThe HDR rendering results are tone-mapped to LDR images using Blender’s AgX tonemapping7. In\\ntotal, we created ∼190,000 random synthetic scenes, resulting in ∼260,000 video clips with at least\\n16 frames per video clip.\\nPerspective crops of HDR panorama images.We use a subset of 1251 HDR panoramas with\\nmeaningful contents from Poly Haven, HDR Maps, and HDRI Skies for the training with perspective\\ncrops. Instead of pre-processing the perspective crops from the HDR panoramas, we do the perspective\\ncrops on-the-fly during the training. The projection camera’s azimuth angle is randomly sampled\\nfrom [0, 360◦] and the elevation angle is randomly sampled from −10◦ to 10◦. The camera’s field\\nof view (FOV) is randomly sampled from 45◦ to 80◦. The perspective crops are rendered with a\\nresolution of 480 × 720. A random tone-mapping function is applied to perspective projection crops'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'start_index': 3135}, page_content='of view (FOV) is randomly sampled from 45◦ to 80◦. The perspective crops are rendered with a\\nresolution of 480 × 720. A random tone-mapping function is applied to perspective projection crops\\n0https://polyhaven.com/\\n1https://hdrmaps.com/\\n2https://hdri-skies.com/\\n3https://doschdesign.com/\\n4http://hdrdb.com/indoor/\\n5http://hdrdb.com/outdoor/\\n6https://huggingface.co/datasets/gvecchio/MatSynth\\n7https://www.blender.org/\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 15, 'page_label': '16', 'start_index': 0}, page_content='to generate LDR images. The tone-mappings include ACES, Filmic, AgX, and Gamma-2.4 sRGB\\nmappings. Auto-exposure (i.e., remapping the 99-th percentile intensity to 0.9) is also randomly\\napplied to the LDR crops. For video input, we create trajectories of projection cameras by smoothly\\nrotating the camera angle within an angular cone of 15◦.\\nPerspective crops of LDR panorama videos.Similar to the perspective crops of HDR panorama\\nimages. We on-the-fly sample perspective crops from the LDR panorama videos. Due to the lack of\\nHDR content, we only apply a random auto-exposure tone-mapping to the perspective crops.\\nB.3 Model Details and Initialization\\nLuxDiT is fine-tuned from the pre-trained CogVideoX-5b-I2V8. To adapt this model for our task,\\nwe replace the original text token with an image input token. This image token is generated in the\\nsame manner as the environment map noise token, but without adding noise. We reuse the model’s'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 15, 'page_label': '16', 'start_index': 749}, page_content='we replace the original text token with an image input token. This image token is generated in the\\nsame manner as the environment map noise token, but without adding noise. We reuse the model’s\\nexisting text-processing layers (e.g., AdaLN) to process these new image input tokens. Furthermore,\\nwe extend the input projection layer to incorporate additional conditioning channels derived from\\nthe concatenated noise token; these extended channels are initialized to zero. Similarly, the output\\nprojection layer is extended to predict dual tone-mapped environment tokens, with its newly added\\nchannels initialized from the original model’s weights.\\nB.4 User Study Details for Virtual Object Insertion\\nFollowing prior works [14, 16, 15, 57, 35], we conduct a user study on Amazon Mechanical Turk to\\ncompare our method against baseline approaches in terms of perceptual realism for virtual object\\ninsertion. Each participant is shown a pair of rendered results—one from our method and one from a'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 15, 'page_label': '16', 'start_index': 1545}, page_content='compare our method against baseline approaches in terms of perceptual realism for virtual object\\ninsertion. Each participant is shown a pair of rendered results—one from our method and one from a\\nbaseline—and asked to assess lighting realism, focusing on shadows, reflections, and overall visual\\nintegration.\\nThe specific instructions shown to participants are:\\nInstruction: Find the inserted virtual object, look at the difference, and select the\\nmore realistic image.\\nAn AI system is trying to insert a virtual object into an image in a natural way.\\nIt aims to make the virtual object look as if it is part of the scene. There are two\\nresults: Trial A and Trial B, and the virtual object is located in the center of each\\nimage. Please zoom in to compare the differences between the two images, and\\npay attention to the lighting effects such as the reflections and shadows.\\nWhich one looks more realistic?\\n□ A\\n□ B'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 15, 'page_label': '16', 'start_index': 2268}, page_content='image. Please zoom in to compare the differences between the two images, and\\npay attention to the lighting effects such as the reflections and shadows.\\nWhich one looks more realistic?\\n□ A\\n□ B\\nParticipants are required to use a monitor 24 inches or larger. Image pairs are randomly shuffled to\\nprevent bias. Following [35], we repeat the user study three times, and recruited 11 unique participants\\nfor each experiment. We compute the percentage of images for which users preferred our method\\nover the baseline, and report the average user preferences for three repeated experiments. In total, the\\nstudy includes 11 × 3 × 11 × 3 = 1089 individual comparisons.\\nB.5 Three-sphere Evaluation Protocol\\nWe adopt the three-sphere rendering setting described in StyleLight [ 54], with evaluation scripts\\nprovided by DiffusionLight9.\\nFor the Laval Indoor dataset, we use the same set of HDR environment maps and corresponding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 15, 'page_label': '16', 'start_index': 3063}, page_content='provided by DiffusionLight9.\\nFor the Laval Indoor dataset, we use the same set of HDR environment maps and corresponding\\nperspective crops as DiffusionLight. We resize and crop the input image to 480 × 720 for our model.\\nFor Laval Outdoor and Poly Haven environment maps, we generate perspective crops using a fixed\\nhorizontal camera with a 60◦ field of view and a resolution of 480 × 720. For Laval Outdoor, we\\napply auto-exposure by scaling the 50th percentile intensity to 0.5.\\n8https://huggingface.co/THUDM/CogVideoX-5b-I2V\\n9https://github.com/DiffusionLight/DiffusionLight-evaluation\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'start_index': 0}, page_content='C Additional Experiments\\nC.1 Array-of-Spheres Evaluation\\nTable 9: Scores on indoor array-of-spheres.\\nMethod si-RMSE ↓ AE ↓\\nEverLight [8] 0.091 6.36\\nStyleLight [54] 0.123 7.09\\nWeber et al. [60] 0.081 4.13\\nEMLight [68] 0.099 3.99\\nDiffusionLight [44] 0.090 5.25\\nOurs 0.089 4.90\\nFollowing prior work [ 60, 8], we evaluate our method\\nusing the array-of-spheres protocol, which renders a grid\\nof diffuse spheres on a ground plane using the predicted\\nenvironment map.\\nWe use 2,240 perspective crops from 224 Laval Indoor\\npanoramas, provided by DiffusionLight10. All input im-\\nages are resized to 512 × 512 to match our model input.\\nQuantitative results are shown in Table 9 and qualitative\\nresults in Fig. 9.\\nWhile our method performs slightly below specialized sys-\\ntems like Weber et al. [60] and EMLight [68], it remains\\ncompetitive—despite not being trained on Laval Indoor.\\nNotably, it outperforms StyleLight [54] and DiffusionLight [44], demonstrating strong generalization\\nacross lighting domains.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'start_index': 817}, page_content='competitive—despite not being trained on Laval Indoor.\\nNotably, it outperforms StyleLight [54] and DiffusionLight [44], demonstrating strong generalization\\nacross lighting domains.\\nInput Image GT DiffusionLight Ours Input Image GT DiffusionLight Ours\\nFigure 9: Visual results on array-of-spheres protocol.\\nC.2 Lighting Estimation with the Cube++ Dataset\\nTable 10: Scores on SpyderCube white face\\nrendering on Cube++ dataset.\\nMethod RMSE ↓ AE ↓\\nLeft Right Left Right\\nD.Light [44] 0.044 0.035 7.221 5.741\\nOurs 0.024 0.025 3.985 4.003\\nWe also evaluated our method on the Cube++\\ndataset [12], specifically designed for illumination esti-\\nmation and color constancy. This dataset includes illu-\\nmination information annotated by the SpyderCube 11.\\nFor our experiment, we selected 100 processed JPEG\\nimages from Cube++. We then applied both Diffusion-\\nLight and our method to estimate the illumination from\\neach image. Subsequently, we rendered the left and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'start_index': 1611}, page_content='images from Cube++. We then applied both Diffusion-\\nLight and our method to estimate the illumination from\\neach image. Subsequently, we rendered the left and\\nright white faces of the SpyderCube under the estimated\\nillumination, assuming purely Lambertian diffuse sur-\\nfaces. To prevent information leakage from the SpyderCube in the input images, we masked out\\nthe SpyderCube from the tested images and inpainted the masked region using LaMa [50]. We then\\ncompared the rendered face colors to the colors sampled directly from the SpyderCube JPEG images.\\nTable 10 presents the RMSE and angular errors, demonstrating that our method clearly outperforms\\nDiffusionLight, achieving angular errors of less than 5◦ on both faces. Visual comparison results are\\nfurther illustrated in Fig. 10.\\n10https://github.com/DiffusionLight/image-array_of_spheres\\n11https://www.datacolor.com/spyder/products/spyder-cube/\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 17, 'page_label': '18', 'start_index': 0}, page_content='GT Ours D.Light GT Ours D.Light GT Ours D.Light GT Ours D.Light GT Ours D.Light\\nFigure 10: Visual results on Cube++ dataset. We show the rendered two white cube faces, mirror ball, and\\nmatte silver ball from our method and DiffusionLight for visual comparison.\\nC.3 Lighting Estimation from Foreground Objects\\nSince our model is trained on object-centric synthetic rendering data, we can also apply it to estimate\\nlighting from foreground objects. We selected 4 NeRF synthetic objects [ 40] and 4 real-world\\nobjects [52], aiming to estimate lighting from videos containing nine consecutive rendering views.\\nWe qualitatively compare LuxDiT with optimization-based inverse rendering methods [41, 18] that\\nreconstruct 3D geometry and lighting from full NeRF scenes. Using the ground truth camera poses,\\nwe rotate each frame’s estimated lighting into the global coordinate system and average across frames\\nto produce the final environment map.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 17, 'page_label': '18', 'start_index': 799}, page_content='we rotate each frame’s estimated lighting into the global coordinate system and average across frames\\nto produce the final environment map.\\nQualitative results are shown in Fig.11. On mostly diffuse objects like lego and hotdog, our method\\nrecovers highlight directions accurately, enabling shadow rendering consistent with the input. For\\nglossy objects like mic and ficus, our model estimates lighting nearly identical to the ground truth.\\nWhile these HDR environment maps are included in our training set, the NeRF scenes differ signifi-\\ncantly from our synthetic renderings (see Fig. 8), indicating that our model leverages shading cues\\nand learned priors rather than direct memorization. In contrast, optimization-based baselines struggle\\nto capture high-frequency lighting detail and often introduce noise and artifacts in lighting.\\nWe further tested our method on real-world foreground objects from the Objects-with-Lighting'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 17, 'page_label': '18', 'start_index': 1542}, page_content='to capture high-frequency lighting detail and often introduce noise and artifacts in lighting.\\nWe further tested our method on real-world foreground objects from the Objects-with-Lighting\\ndataset [52], which provides ground truth distant environment lighting. Similar to the NeRF synthetic\\nscene setup, the estimated lighting was then aligned into the global coordinate system using ground\\ntruth camera poses. We compared our approach to NeuS+Mitsuba [ 55, 25], the top-performing\\nmethod on this dataset [52]. The metrics, using the three-sphere protocol, are presented in Table 11,\\nwith visual results in Fig. 12.\\nWhile our model performs well overall, minor errors remain, e.g. color shifts in the NeRF Lego\\nscene (Fig. 11) and a slightly higher si-RMSE compared to NeuS+Mitsuba (Table 11). We believe\\ncombining our generative model with optimization-based methods could further enhance lighting\\nestimation, which we leave for future work.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 17, 'page_label': '18', 'start_index': 2346}, page_content='combining our generative model with optimization-based methods could further enhance lighting\\nestimation, which we leave for future work.\\nTable 11: Comparison of our method with NeuS+Mitsuba on Objects with Lighting datasets.\\nMethod Scale-invariant RMSE↓ Angular Error↓ Normalized RMSE↓\\nDiffuse Matte Mirror Diffuse Matte Mirror Diffuse Matte Mirror\\nNeuS+Mitsuba 0.082 0.232 0.424 3.145 3.383 3.526 0.180 0.545 0.717\\nOurs 0.086 0.253 0.482 1.262 1.594 2.000 0.153 0.339 0.479\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 18, 'page_label': '19', 'start_index': 0}, page_content='GT & Input NVDIFFREC [41] NVDIFFRECMC [18] Ours\\nFigure 11: Lighting estimation from the NeRF synthetic objects. We use the estimated lighting from different\\nmethods to re-render the original NeRF Blender scenes.\\nOurs NeuS+Mitsuba\\nInput GT\\nOurs NeuS+Mitsuba\\nInput GT\\n Input GT\\nOurs NeuS+Mitsuba\\nFigure 12: Lighting estimation from the masked real objects from Objects with Lighting.\\nC.4 Additional Ablations\\nC.4.1 The Choice of the HDR Fusion Model\\nAs detailed in Sec. 4.1, a lightweight MLP ψ is employed to merge the dual-tonemapped environment\\nmaps, Eldr and Elog, thereby reconstructing the HDR environment map ˆE. There are also alternative\\nfusion methods, such as using a more complex CNN model to incorporate adjacent pixel information\\nfor HDR fusion, or applying a rule-based approach with explicit inverse equations. To justify our\\nchoice of a simple MLP, we evaluate various HDR fusion techniques, including MLP, CNN, and a'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 18, 'page_label': '19', 'start_index': 742}, page_content='for HDR fusion, or applying a rule-based approach with explicit inverse equations. To justify our\\nchoice of a simple MLP, we evaluate various HDR fusion techniques, including MLP, CNN, and a\\nrule-based method. The CNN model has an identical number of layers to our MLP model, using3 ×3\\nconvolution kernels across layers. The rule-based method involves applying the inverse Reinhard map\\nfor lights with intensity below 8, a linear interpolation between Reinhard and log maps for intensities\\nranging from 8 to 16, and exclusively the log map for intensities exceeding 16.\\nTable 12: Comparison on differ-\\nent HDR fusion approaches.\\nMLP CNN Rule\\nRMSE↓ 11.55 11.74 11.71\\nTable 12 presents the RMSE results on testing Polyhaven HDRIs.\\nAll three methods demonstrate comparable accuracy, with the MLP\\napproach exhibiting a slight advantage. Compared to the rule-based\\napproach, we believe the neural approach can better handle numer-\\nical inconsistency after image uint8 quantization, and the potential'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 18, 'page_label': '19', 'start_index': 1602}, page_content='approach, we believe the neural approach can better handle numer-\\nical inconsistency after image uint8 quantization, and the potential\\ndata range overflow (e.g., lights beyond the pre-defined maximum\\nintensity 10000).\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', 'start_index': 0}, page_content='C.4.2 The Impact of LoRA on Synthetic Scenes\\nSection 5.5 demonstrates the impact of varying LoRA scales (0.0 to 1.0) on the predicted lighting\\ncontent of real-world images. This ablation study, conversely, investigates how our LoRA model,\\ntrained with real images, affects the lighting estimation of synthetic foreground objects. Table 13\\npresents the angular errors using a three-sphere evaluation, and Fig. 16 provides the visual results.\\nTable 13: Ablation study on impact of LoRA scale on synthetic foreground objects.\\nLoRA Scale 0.00 0.25 0.50 0.75 1.00\\nDiffuse ↓ 1.594 1.737 2.170 3.832 3.937\\nMatte ↓ 2.068 2.311 2.914 5.322 5.891\\nMirror ↓ 3.405 3.690 4.342 6.783 7.400\\nIn contrast to the ablation performed on scene images, a larger LoRA scale leads to lower lighting\\nestimation accuracy. As Fig. 16 illustrates, increasing the LoRA scale causes foreground content\\nto gradually appear on the estimated environment map, which is consistent with our LoRA model’s'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', 'start_index': 775}, page_content='estimation accuracy. As Fig. 16 illustrates, increasing the LoRA scale causes foreground content\\nto gradually appear on the estimated environment map, which is consistent with our LoRA model’s\\nbehavior. Nevertheless, the estimated highlights remain consistent across different LoRA scales.\\nD Additional Results\\nWe provide additional visual results in this section to further support the claims made in the main\\npaper.\\n• Model Ablation and LoRA Scale: Figure 13 details the ablation study on our model’s design\\nand the exploration of different LoRA scales.\\n• Camera Parameter Variations: Figures 14 and 15 show lighting estimation performance\\nwhen varying camera field of view (FOV) and elevation angles, respectively.\\n• Three-Sphere Rendering Evaluations: Figures 17, 18, and 19 display further lighting estima-\\ntion outcomes using the three-sphere rendering protocol on the Laval Indoor, Laval Outdoor,\\nand Poly Haven datasets.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', 'start_index': 1587}, page_content='tion outcomes using the three-sphere rendering protocol on the Laval Indoor, Laval Outdoor,\\nand Poly Haven datasets.\\n• Virtual Object Insertion: Figures 20 and 21 illustrate additional virtual object insertion\\nresults on Poly Haven panorama crops and Waymo driving scenes.\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 20, 'page_label': '21', 'start_index': 0}, page_content='Input Image Model Ablation LoRA Scale Exploration\\nchannel concat. LoRA 0.0 LoRA 0.1 LoRA 0.2 LoRA 0.4\\nw/o syn. data LoRA 0.6 LoRA 0.7 LoRA 0.8 LoRA 1.0\\nFigure 13: Model design ablation and LoRA scale exploration. The “Model Ablation” column\\nshows the results of our two model design variants: 1) channel concatenation and 2) training without\\nsynthetic rendering data. The “LoRA Scale Exploration” columns show the visual results of our\\nmodel with different LoRA scales.\\nFOV 45◦ 50◦ 60◦ 70◦ 75◦\\nGT\\nFigure 14: Lighting estimation from input images with varying camera FOV .\\nElevation 30◦ ↑ 15◦ ↑ 0◦ 15◦ ↓ 30◦ ↓\\nGT\\nFigure 15: Lighting estimation from input images with varying camera elevation.\\nInput Image LoRA Scale Exploration\\nGT LoRA 0.00 LoRA 0.25\\nLoRA 0.50 LoRA 0.75 LoRA 1.00\\nFigure 16: LoRA scale exploration on synthetic foreground scenes.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22', 'start_index': 0}, page_content='Input Image GT StyleLight DiffusionLight Ours\\nFigure 17: Additional qualitative results on Laval Indoor dataset.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23', 'start_index': 0}, page_content='Input Image GT H-G et al. NLFE DiffusionLight Ours\\nFigure 18: Additional qualitative results on Laval Outdoor dataset.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 23, 'page_label': '24', 'start_index': 0}, page_content='Input Image GT StyleLight NLFE DiffusionLight Ours\\nFigure 19: Additional qualitative results on Poly Haven dataset.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 24, 'page_label': '25', 'start_index': 0}, page_content='Reference StyleLight DiffusionLight DiPIR Ours\\nFigure 20: Additional virtual object insertion on Poly Haven perspective crops.\\nH-G et al. [22] NLFE DiffusionLight DiPIR Ours\\nFigure 21: Additional virtual object insertion on Waymo driving scenes.\\n25')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95b84147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5a1bb",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "REFE: https://platform.openai.com/docs/guides/embeddings/embedding-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc578c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd2f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6ebe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "599b5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "embb_1 = embeddings.embed_query(\"I am learning LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6c4e8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embb_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89befa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embb_2 = embeddings.embed_query(\"How are you\")\n",
    "len(embb_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "822441a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.021494783461093903,\n",
       "  0.0489584244787693,\n",
       "  -0.026290571317076683,\n",
       "  0.02522100694477558,\n",
       "  0.015525927767157555,\n",
       "  -0.006348379421979189,\n",
       "  -0.0041898442432284355,\n",
       "  0.0436796098947525,\n",
       "  -0.020476972684264183,\n",
       "  0.038262784481048584],\n",
       " [0.0001367140212096274,\n",
       "  0.02260086126625538,\n",
       "  -0.013737630099058151,\n",
       "  -0.004893321078270674,\n",
       "  0.004681390710175037,\n",
       "  0.0048441230319440365,\n",
       "  -0.01318509690463543,\n",
       "  0.006338988896459341,\n",
       "  -0.007008840329945087,\n",
       "  0.032697830349206924])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embb_1[:10], embb_2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a6f85",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47da164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import  FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b044f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(len(embb_2))\n",
    "vector_store = FAISS(embedding_function=embeddings, \n",
    "                    index=index,\n",
    "                    docstore=InMemoryDocstore(),\n",
    "                    index_to_docstore_id={}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "244a5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c92a157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8b7f80bb-2ee0-4b71-bd74-44f65bd671b1', metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'start_index': 0}, page_content='Input Image Lighting Estimation Virtual Object Insertion\\nFigure 1: LuxDiT is a generative lighting estimation model that predicts high-quality HDR envi-\\nronment maps from visual input. It produces accurate lighting while preserving scene semantics,\\nenabling realistic virtual object insertion under diverse conditions.\\nfor producing reliable lighting estimates in a single inference and cannot directly generate HDR\\noutputs. As a result, DiffusionLight relies on an expensive test-time ensemble strategy to improve\\nrobustness. Moreover, sampling multiple exposures through separate inference passes introduces\\ninconsistencies and limits the dynamic range of the reconstructed illumination.\\nIn this work, we formulate lighting estimation as a conditional generative task and propose LuxDiT,\\na neural lighting predictor trained on synthetic data and adapted to real-world scenes. Conditioned on\\nvisual input, our approach fine-tunes a diffusion transformer (DiT) to synthesize HDR panoramas'),\n",
       " Document(id='5a713617-d633-4e1e-808a-6f3941ebe637', metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'start_index': 3098}, page_content='demonstrating robustness to moderate viewpoint shifts.\\n6 Discussion\\nWe introduce LuxDiT, a conditional generative model for estimating HDR scene illumination from\\ncasually captured images and videos. Our approach fine-tunes a video diffusion transformer (DiT)\\nto synthesize HDR environment maps, combining large-scale synthetic data for learning physically\\ngrounded priors with LoRA-based adaptation on real HDR panoramas to improve semantic alignment.\\nExtensive experiments demonstrate that LuxDiT produces accurate, high-frequency, and scene-\\nconsistent lighting predictions from limited visual input.\\nLimitations and future work.While LuxDiT produces high-quality lighting predictions, inference\\nremains computationally intensive due to the iterative nature of diffusion models, limiting its use in\\nreal-time applications. Future work could explore model distillation or more efficient architectures'),\n",
       " Document(id='b0f2976e-b0b6-4d99-b0d6-2fb6f89d196d', metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'start_index': 1613}, page_content='light direction and intensity. While this imparts general lighting priors, models trained purely on\\nsynthetic data often hallucinate lighting based on dataset priors, producing environment maps that\\nare plausible but semantically mismatched with the input scene. For example, an image of an urban\\nstreet may yield an environment map depicting a rural landscape. To address this, we further apply\\nlow-rank adaptation (LoRA) [23] on a curated set of real HDR panoramas, improving alignment\\nbetween predicted lighting and scene semantics.\\nGiven a single image or video, LuxDiT produces HDR environment maps with accurate direction,\\nintensity, and scene-consistent content. It reduces lighting estimation error by 45% on Laval Outdoor\\nsunlight direction and improves temporal consistency for video input, enabling reliable use in\\ndownstream applications such as virtual object insertion. Our main contributions are:'),\n",
       " Document(id='a325e71c-c3d6-4c60-a7a5-ebd47dfdb946', metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Ruofan Liang; Kai He; Zan Gojcic; Igor Gilitschenski; Sanja Fidler; Nandita Vijaykumar; Zian Wang', 'doi': 'https://doi.org/10.48550/arXiv.2509.03680', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'LuxDiT: Lighting Estimation with Video Diffusion Transformer', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2509.03680v1', 'source': 'A:\\\\Indian_AI_Production_5\\\\agentic_ai\\\\agentic_ai_projects\\\\data\\\\pdfs\\\\2509.03680v1.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'start_index': 3384}, page_content='can be the noise ϵ [20], the v-prediction √¯αtϵ − √1 − ¯αtx0 [47], or the clean signal x0 itself [27].\\nAt inference time, samples are generated by denoising an initial Gaussian sample through a fixed\\nnumber of reverse steps. In this paper, we build on CogVideoX [62], a latent video diffusion model\\ntrained on compressed video representations. A pretrained auto-encoder pair {E, D} maps RGB\\nvideos to and from a latent space, such that E(x) = z and D(z) ≈ x. All diffusion training and\\ngeneration is performed in this lower-dimensional latent space to reduce memory and computational.\\n4 Method\\nWe propose LuxDiT, a diffusion-based generative framework for estimating high-dynamic-range\\n(HDR) environment maps from a single image or video. We tailor a recent video diffusion transformer\\narchitecture [62] for lighting estimation, by jointly processing denoising targets (environment lighting)\\nand condition tokens (LDR input images) through self-attention layers. Since a single image can be')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"What is LuxDiT\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d06f329e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='832b4a01-4f2d-462b-8a4c-12751984a029', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 10685}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
       " Document(id='2c1619cd-0d65-4523-b633-caff04bf643b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 11585}, page_content='Categorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.'),\n",
       " Document(id='dd1878f3-fb80-4ec0-b731-8f4b2817cbe4', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 9720}, page_content='Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).'),\n",
       " Document(id='7e1b9969-4f02-44eb-aa44-4be0cb27f6d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 971}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "vector_store.similarity_search(\"what is Short-term memory\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1609aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb_path = r\"A:\\Indian_AI_Production_5\\agentic_ai\\agentic_ai_projects\\data\\faiss_vdb\"\n",
    "vector_store.save_local(vdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e057e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb_path = r\"A:\\Indian_AI_Production_5\\agentic_ai\\agentic_ai_projects\\data\\faiss_vdb\"\n",
    "vector_store_local = FAISS.load_local(vdb_path, embeddings=embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9faa6fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='832b4a01-4f2d-462b-8a4c-12751984a029', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 10685}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
       " Document(id='2c1619cd-0d65-4523-b633-caff04bf643b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 11585}, page_content='Categorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.'),\n",
       " Document(id='dd1878f3-fb80-4ec0-b731-8f4b2817cbe4', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 9720}, page_content='Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).'),\n",
       " Document(id='7e1b9969-4f02-44eb-aa44-4be0cb27f6d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 971}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_local.similarity_search(\"What is Short-term memory\", k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a609f",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc62061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ad4386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=[\"context\", \"question\"],\n",
    "                template=RAG_PROMPT_TEMPLATE,\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt\n",
    "\n",
    "\n",
    "### or \n",
    "\n",
    "# prompt = hub.pull('rlm/rag-prompt') ## REF: https://smith.langchain.com/hub\n",
    "# prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b146d",
   "metadata": {},
   "source": [
    "## Create RAG AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe4af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict, List\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a01d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(model='gpt-4.1-nano', model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "383578a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_1f35c1788c', 'id': 'chatcmpl-CPBG9llx4hV8afedCZXMBz17Dnkj5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ff9276ec-9bd7-4c70-a2ab-187d1ee1b469-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a2e49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa9e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state:State):\n",
    "    retrived_docs = vector_store_local.similarity_search(state['question'], k=4)\n",
    "    return {'context': retrived_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c984672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content  for doc in state['context'])\n",
    "    messages = prompt.invoke({'question': state['question'], 'context': docs_content})\n",
    "    responce = llm.invoke(messages)\n",
    "    return {'answer': responce.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e70098e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_node(\"generator\", generator)\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\", \"generator\")\n",
    "graph_builder.add_edge(\"generator\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9c76797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000262FEFBE390>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b1f0c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tretrieve(retrieve)\n",
      "\tgenerator(generator)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> retrieve;\n",
      "\tretrieve --> generator;\n",
      "\tgenerator --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(graph.get_graph().draw_mermaid()) ## REF: https://mermaid.live/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b82eb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Short-term memory',\n",
       " 'context': [Document(id='832b4a01-4f2d-462b-8a4c-12751984a029', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 10685}, page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:'),\n",
       "  Document(id='2c1619cd-0d65-4523-b633-caff04bf643b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 11585}, page_content='Categorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.'),\n",
       "  Document(id='dd1878f3-fb80-4ec0-b731-8f4b2817cbe4', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 9720}, page_content='Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).'),\n",
       "  Document(id='7e1b9969-4f02-44eb-aa44-4be0cb27f6d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 971}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.')],\n",
       " 'answer': 'Short-term memory (STM) or working memory stores information that we are currently aware of and use for tasks like learning and reasoning. It has a limited capacity of about 7 items and lasts for approximately 20-30 seconds. It is associated with in-context learning and temporary information retention.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({'question': 'What is Short-term memory'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c7e88d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Indian AI Production?',\n",
       " 'context': [Document(id='7725e059-826e-407f-bb7a-1ce8c9783cc3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 26585}, page_content='The generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:'),\n",
       "  Document(id='69abd96c-0ace-428d-a7bd-76a046e7b275', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 20401}, page_content='API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.'),\n",
       "  Document(id='c4a86f95-32fe-407c-89c4-f10c684eed97', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       "  Document(id='9fa8a90b-667e-408b-b3da-adfda92e67b0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 14939}, page_content='Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)')],\n",
       " 'answer': 'The context does not provide specific information about \"Indian AI Production.\" Therefore, I don\\'t know what Indian AI Production refers to.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({'question': 'What is Indian AI Production?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7464fc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
